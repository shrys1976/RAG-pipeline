{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "203f2cf4",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.documents import Document\n",
        "doc = Document(\n",
        "\n",
        "    page_content = \"some content\",\n",
        "    metadata = {\n",
        "        \"source\":\"text.txt\",\n",
        "        \"pages\":\"1\",\n",
        "        \"author\":\"shrys\",\n",
        "        \"data_created\":\"12-2-26\"\n",
        "    }\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "bb848266",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os \n",
        "os.makedirs(\"data/text_files\",exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "e480b571",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample text file written.\n"
          ]
        }
      ],
      "source": [
        "sample_texts={\n",
        "    \"../data/text_files/python_intro.txt\":\"\"\"Python Programming Introduction\n",
        "\n",
        "Python is a high-level, interpreted programming language known for its simplicity and readability.\n",
        "Created by Guido van Rossum and first released in 1991, Python has become one of the most popular\n",
        "programming languages in the world.\n",
        "\n",
        "Key Features:\n",
        "- Easy to learn and use\n",
        "- Extensive standard library\n",
        "- Cross-platform compatibility\n",
        "- Strong community support\n",
        "\n",
        "Python is widely used in web development, data science, artificial intelligence, and automation.\"\"\",\n",
        "    \n",
        "    \"../data/text_files/machine_learning.txt\": \"\"\"Machine Learning Basics\n",
        "\n",
        "Machine learning is a subset of artificial intelligence that enables systems to learn and improve\n",
        "from experience without being explicitly programmed. It focuses on developing computer programs\n",
        "that can access data and use it to learn for themselves.\n",
        "\n",
        "Types of Machine Learning:\n",
        "1. Supervised Learning: Learning with labeled data\n",
        "2. Unsupervised Learning: Finding patterns in unlabeled data\n",
        "3. Reinforcement Learning: Learning through rewards and penalties\n",
        "\n",
        "Applications include image recognition, speech processing, and recommendation systems\n",
        "    \n",
        "    \n",
        "    \"\"\"\n",
        "\n",
        "}\n",
        "\n",
        "for filepath, content in sample_texts.items():\n",
        "    with open(filepath, 'w', encoding=\"utf-8\") as f:\n",
        "        f.write(content)\n",
        "\n",
        "print(\"Sample text file written.\")        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "96a6e30c",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\code\\RAG-pipeline\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "# using textloader from langchain\n",
        "\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "loader = TextLoader(\"../data/text_files/python_intro.txt\",encoding = \"utf-8\" )\n",
        "document = loader.load()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "e8ea1836",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Document(metadata={'source': '../data/text_files/python_intro.txt'}, page_content='Python Programming Introduction\\n\\nPython is a high-level, interpreted programming language known for its simplicity and readability.\\nCreated by Guido van Rossum and first released in 1991, Python has become one of the most popular\\nprogramming languages in the world.\\n\\nKey Features:\\n- Easy to learn and use\\n- Extensive standard library\\n- Cross-platform compatibility\\n- Strong community support\\n\\nPython is widely used in web development, data science, artificial intelligence, and automation.')]\n"
          ]
        }
      ],
      "source": [
        "print(document)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "4679db18",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Document(metadata={'source': '..\\\\data\\\\text_files\\\\machine_learning.txt'}, page_content='Machine Learning Basics\\n\\nMachine learning is a subset of artificial intelligence that enables systems to learn and improve\\nfrom experience without being explicitly programmed. It focuses on developing computer programs\\nthat can access data and use it to learn for themselves.\\n\\nTypes of Machine Learning:\\n1. Supervised Learning: Learning with labeled data\\n2. Unsupervised Learning: Finding patterns in unlabeled data\\n3. Reinforcement Learning: Learning through rewards and penalties\\n\\nApplications include image recognition, speech processing, and recommendation systems\\n\\n\\n    '), Document(metadata={'source': '..\\\\data\\\\text_files\\\\python_intro.txt'}, page_content='Python Programming Introduction\\n\\nPython is a high-level, interpreted programming language known for its simplicity and readability.\\nCreated by Guido van Rossum and first released in 1991, Python has become one of the most popular\\nprogramming languages in the world.\\n\\nKey Features:\\n- Easy to learn and use\\n- Extensive standard library\\n- Cross-platform compatibility\\n- Strong community support\\n\\nPython is widely used in web development, data science, artificial intelligence, and automation.')]\n"
          ]
        }
      ],
      "source": [
        "# Directory loader from langchain\n",
        "from langchain_community.document_loaders import DirectoryLoader, directory\n",
        "dir_loader  = DirectoryLoader(\n",
        "\n",
        "    \"../data/text_files\",\n",
        "    glob = \"**/*.txt\", # pattern to match\n",
        "    loader_cls = TextLoader, # loader class to use\n",
        "    loader_kwargs = {'encoding':'utf-8'},\n",
        "    show_progress=False\n",
        ")\n",
        "\n",
        "documents  = dir_loader.load()\n",
        "print(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "6d41c96a",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:57610bf)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'total_pages': 19, 'format': 'PDF 1.7', 'title': 'Beyond the Loss Curve: Scaling Laws, Active Learning, and the Limits of Learning from Exact Posteriors', 'author': 'Arian Khorasani; Nathaniel Chen; Yug D Oswal; Akshat Santhana Gopalan; Egemen Kolemen; Ravid Shwartz-Ziv', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 0}, page_content='Beyond the Loss Curve: Scaling Laws, Active Learning, and the Limits of\\nLearning from Exact Posteriors\\nArian Khorasani 1 Nathaniel Chen * 2 Yug D Oswal * 3 Akshat Santhana Gopalan 4 Egemen Kolemen 2\\nRavid Shwartz-Ziv 5\\nAbstract\\nHow close are neural networks to the best they\\ncould possibly do? Standard benchmarks can-\\nnot answer this because they lack access to the\\ntrue posterior p(y|x). We use class-conditional\\nnormalizing flows as oracles that make exact pos-\\nteriors tractable on realistic images (AFHQ, Im-\\nageNet). This enables five lines of investigation.\\nScaling laws: Prediction error decomposes into ir-\\nreducible aleatoric uncertainty and reducible epis-\\ntemic error; the epistemic component follows a\\npower law in dataset size, continuing to shrink\\neven when total loss plateaus. Limits of learn-\\ning: The aleatoric floor is exactly measurable,\\nand architectures differ markedly in how they ap-\\nproach it: ResNets exhibit clean power-law scal-\\ning while Vision Transformers stall in low-data\\nregimes. Soft labels: Oracle posteriors contain\\nlearnable structure beyond class labels: training\\nwith exact posteriors outperforms hard labels and\\nyields near-perfect calibration. Distribution shift:\\nThe oracle computes exact KL divergence of con-\\ntrolled perturbations, revealing that shift type mat-\\nters more than shift magnitude: class imbalance\\nbarely affects accuracy at divergence values where\\ninput noise causes catastrophic degradation. Ac-\\ntive learning: Exact epistemic uncertainty dis-\\ntinguishes genuinely informative samples from\\ninherently ambiguous ones, improving sample ef-\\nficiency. Our framework reveals that standard\\nmetrics hide ongoing learning, mask architectural\\ndifferences, and cannot diagnose the nature of\\ndistribution shift.\\n*Equal contribution 1Mila-Quebec AI Institute, Canada 2Plasma\\nPhysics Lab, Princeton University, USA 3School of Computer Sci-\\nence and Engineering, Vellore Institute of Technology, India 4High\\nSchool Student, John P. Stevens High School, USA 5Center of Data\\nScience, New York University, New York, USA. Correspondence\\nto: Arian Khorasani <Arian.Khorasani@mila.quebec>.\\nPreprint. February 3, 2026.\\n1. Introduction\\nEvery paper reporting test accuracy implicitly asks: how\\ngood is this model? But good compared to what? Without\\naccess to the true posterior p(y|x), we cannot tell whether a\\nmodel is at 50% or 99% of the theoretical maximum. When\\na loss curve flattens, we cannot distinguish irreducible noise\\n(aleatoric uncertainty) from gaps in the model’s knowledge\\n(epistemic uncertainty) (Gal & Ghahramani, 2016; Guo\\net al., 2017). Neural Scaling laws (Kaplan et al., 2020) tell\\nus loss decreases as N −α, but loss conflates both (Hoffmann\\net al., 2022). When performance degrades under distribu-\\ntion shift, we cannot tell whether the shift was large or\\nsmall (Hendrycks & Dietterich, 2019) (Gal & Ghahramani,\\n2016).\\nA class-conditional normalizing flow (Dinh et al., 2017;\\nKingma & Dhariwal, 2018) trained on real images provides\\na concrete reference point. The flow defines an explicit\\npθ(x | y) from which we can both sample and evaluate\\nexact likelihoods. We treat this not as an approximation\\nto nature, but as the complete specification of a synthetic\\nworld in which Bayes-optimal posteriors are computable in\\nclosed form (Dinh et al., 2017). In this world, the expected\\ncross-entropy of any classifier decomposes exactly:\\nL(qθ) = Ex[H(p(y|x))]\\n|\\n{z\\n}\\naleatoric (irreducible)\\n+ Ex[KL(p(y|x)∥qθ(y|x))]\\n|\\n{z\\n}\\nepistemic (reducible)\\nThis decomposition, intractable on any standard benchmark,\\nis a direct consequence of Bayes-optimal risk decomposition\\nand is the basis for all our experiments. As illustrated in\\nFigure 1, we train state-of-the-art normalizing flows on\\nrealistic image datasets and conduct a thorough validation\\nto confirm the generated images match real data statistics\\nand are not memorized (Section 2.3). We find:\\n• Scaling laws beyond the loss curve. Epistemic error\\nfollows a power law KL ∝N −α that continues to\\nshrink even when total loss plateaus. The exponents\\ndiffer across architectures.\\n• The limits of learning are exactly measurable. The\\naleatoric floor is a hard limit no architecture can beat.\\n1\\narXiv:2602.00315v1  [cs.LG]  30 Jan 2026'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:57610bf)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'total_pages': 19, 'format': 'PDF 1.7', 'title': 'Beyond the Loss Curve: Scaling Laws, Active Learning, and the Limits of Learning from Exact Posteriors', 'author': 'Arian Khorasani; Nathaniel Chen; Yug D Oswal; Akshat Santhana Gopalan; Egemen Kolemen; Ravid Shwartz-Ziv', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 1}, page_content='Decomposing Neural Network Error via Flow-Based Oracles\\nFigure 1. Normalizing flows enable exact posterior computation on realistic images. (Left) We train class-conditional flows on\\nAFHQ/ImageNet, creating a frozen oracle with tractable likelihoods. (Center) The oracle decomposes prediction error into aleatoric\\n(irreducible) and epistemic (reducible) components, quantities hidden in standard benchmarks. (Right) We apply this framework to\\ndiagnose scaling laws, quantify distribution shift with exact KL divergence, and train models with exact soft labels.\\nArchitectures differ markedly in how they approach it:\\nResNets show clean power-law scaling while Vision\\nTransformers stall without pretraining.\\n• Soft labels from exact posteriors enable better learn-\\ning. Training with p(y|x) instead of argmax labels\\noutperforms hard labels and yields near-perfect calibra-\\ntion (Szegedy et al., 2015).\\n• Distribution shift is exactly quantifiable. The oracle\\ncomputes KL[pshifted∥pbaseline] exactly, revealing that\\nshift type matters far more than magnitude: class im-\\nbalance barely affects accuracy at divergence values\\nwhere input noise causes collapse.\\n• Active learning with ground-truth uncertainty. Ex-\\nact epistemic uncertainty distinguishes genuinely in-\\nformative samples from inherently ambiguous ones,\\nimproving sample efficiency over standard acquisition\\nfunctions that confound aleatoric and epistemic uncer-\\ntainty (Gal et al., 2017).\\nOur oracle defines a synthetic world, not nature. But the phe-\\nnomena it reveals (hidden epistemic progress, architectural\\nscaling gaps, misleading divergence measures) are proper-\\nties of the models and metrics, not the data source (Belghazi\\net al., 2021).\\n2. The Oracle Framework\\nWe build an oracle benchmark by training class-conditional\\nnormalizing flows that define tractable densities pθ(x | y) on\\nhigh-dimensional inputs (Papamakarios et al., 2021). Once\\nthe flows are trained and frozen, they specify a synthetic\\nworld in which the posterior is available by Bayes’ rule,\\np(y | x) =\\npθ(x | y) πy\\nP\\ny′ pθ(x | y′) πy′ .\\n(1)\\nBecause likelihoods are tractable, we can compute Bayes-\\noptimal posteriors exactly under the oracle model (up to\\nfloating-point arithmetic) via Bayes’ rule (Dinh et al., 2017).\\nThis in turn makes our loss decomposition and controlled\\nscaling, shift, and active-learning experiments possible.\\nPractical details on training and scaling to many classes\\n(ImageNet) are provided in the appendix (Deng et al., 2009).\\n2.1. Flows as Oracles\\nA normalizing flow defines an invertible mapping between\\ninputs x and latent variables z (Dinh et al., 2017; Kingma &\\nDhariwal, 2018): the forward map fθ sends x 7→z, while\\nthe inverse f −1\\nθ\\ngenerates samples by mapping z 7→x for\\nz ∼N(0, I). The key property is that densities are tractable\\n(Papamakarios et al., 2021):\\nlog pθ(x) = log pZ(fθ(x)) + log |det Jfθ(x)| .\\n(2)\\n2'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:57610bf)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'total_pages': 19, 'format': 'PDF 1.7', 'title': 'Beyond the Loss Curve: Scaling Laws, Active Learning, and the Limits of Learning from Exact Posteriors', 'author': 'Arian Khorasani; Nathaniel Chen; Yug D Oswal; Akshat Santhana Gopalan; Egemen Kolemen; Ravid Shwartz-Ziv', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 2}, page_content='Decomposing Neural Network Error via Flow-Based Oracles\\nWe train one flow per class, yielding class-conditional densi-\\nties pθ(x | y), and then freeze the parameters. For any input\\nx, we compute the oracle posterior via Bayes’ rule (Eq. 1).\\nUnlike typical Bayesian methods that rely on sampling or\\nvariational approximations, this computation is exact under\\nthe oracle model (Alemi et al., 2019) since the likelihoods\\nand class prior are explicitly known: given the trained flows\\nand the chosen class prior π, p(y | x) is computed in closed\\nform (implemented in log space with log-sum-exp normal-\\nization for numerical stability). The oracle therefore returns\\nthe true posterior for the synthetic world defined by the flow.\\n2.2. What Does “Truth” Mean Here?\\nHere, “true” means Bayes-optimal with respect to the oracle\\nworld induced by the trained flows. The oracle world is\\ndefined by (i) the per-class flow densities pθ(x | y), (ii) the\\nexact preprocessing pipeline used during flow training and\\nsampling, and (iii) a class prior πy. All “exact” statements\\nare with respect to this specification: for the oracle world,\\nwe can compute the Bayes posterior p(y | x) (Eq. 1) and\\ntherefore quantities such as the aleatoric term E[H(p(y |\\nx))] and epistemic term E[KL(p ∥qθ)] exactly under the\\noracle model (Section 3).\\nUnless otherwise stated, we use a uniform prior over classes\\n(πy = 1/K). In the class-imbalance shift experiments, we\\nmodify the class prior π while keeping pθ(x | y) fixed\\n(Chawla et al., 2002), isolating prior shift from conditional\\nshift.\\nThe main limitation is external validity: conclusions are\\nonly useful insofar as samples from the oracle world match\\nthe statistical and semantic properties of the real data used\\nto fit the flows.\\n2.3. Oracle Validation\\nBottom line: Generated images are statistically similar\\nto the original data across six metrics, and classifiers\\ntrained on oracle samples approach the Bayes-optimal\\nbound (Salimans et al., 2016).\\nBecause the oracle provides exact posteriors, we can com-\\npute the Bayes-optimal accuracy for the oracle world:\\n99.77% on AFHQ (Choi et al., 2020) (Bayes error 0.23%).\\nIf classifiers trained on oracle data approach this bound,\\nthe oracle is producing learnable, well-structured sam-\\nples. We trained five architectures on 50K oracle samples.\\nConvNeXt (Liu et al., 2022) reaches 98.0%, ResNet (He\\net al., 2015) 97.7%, Swin (Liu et al., 2021) 97.7%, Mo-\\nbileNet (Howard et al., 2019) 97.7%, and ViT-Base (Doso-\\nvitskiy et al., 2021) 97.0% (Table 1).\\nThe remaining\\n1.8–2.8% gap closes with longer training. On ImageNet-\\n64 (Chrabaszcz et al., 2017), the Bayes error is 4.031%.\\nTable 1. Classifiers approach the Bayes-optimal bound on\\noracle data. Self-validation on AFHQ: all architectures reach 97–\\n98% accuracy, within 2–3% of the theoretical optimum (99.8%).\\nArchitecture\\nAccuracy (%)\\nError (%)\\nGap (%)\\nConvNeXt\\n98.03\\n1.97\\n1.75\\nResNet-18\\n97.73\\n2.27\\n2.05\\nSwin\\n97.68\\n2.32\\n2.10\\nMobileNet\\n97.65\\n2.35\\n2.12\\nViT-Base\\n96.98\\n3.02\\n2.80\\nWe validated distributional quality using six metrics.\\nFID (Heusel et al., 2018) measures feature distribution dis-\\ntance (28.44 on AFHQ, 13.48 on ImageNet-64), higher\\nthan diffusion models because flows trade sharpness for ex-\\nact likelihoods. Inception Score (Salimans et al., 2016)\\nevaluates quality and diversity (6.24 ± 3.57 on AFHQ,\\n32.57 ± 4.47 on ImageNet-64). Manifold coverage con-\\nfirms broad support (90% on AFHQ, 89.9% on ImageNet-\\n64). Feature variance match shows semantic diversity\\nis preserved (83–92% on AFHQ, 72–93% on ImageNet-\\n64). Memorization check: only 36% of AFHQ samples\\n(6.5% on ImageNet-64) have a training neighbor within fea-\\nture distance 10, and visual inspection confirms these share\\npose/lighting but depict distinct individuals. Full metrics\\nand protocol in Appendix A, Table 4.\\n2.4. Implementation\\nWe use TarFlow (Zhai et al., 2025), a recent normalizing\\nflow that achieves strong likelihood scores on AFHQ and Im-\\nageNet. Training follows standard practice: dequantization,\\nlogit preprocessing, and maximum likelihood optimization.\\nWe train separate flows per class on AFHQ (3 classes: cat,\\ndog, wild) and ImageNet (Deng et al., 2009) (up to 1000\\nclasses at 64×64 and 128×128 resolution). After training,\\nwe freeze the flows and generate 50,000 labeled samples\\nwith oracle posteriors. We compute posteriors in log space\\nand normalize with log-sum-exp for numerical stability. Full\\narchitecture and systems details (including throughput and\\nscaling across many classes) appear in the appendix.\\n3. Decomposing Prediction Error\\nWhy does this decomposition matter? In practice, a flatten-\\ning loss curve is ambiguous: it could mean the model has\\nlearned everything learnable, or it could mean the model is\\nstuck while reducible error remains. Distinguishing these\\ncases determines whether more data, more capacity, or a\\ndifferent task formulation is needed. Similarly, calibration\\nmethods aim to match model confidence to true probabil-\\nities, but without access to true posteriors, calibration can\\nonly be evaluated against empirical frequencies (Guo et al.,\\n3'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:57610bf)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'total_pages': 19, 'format': 'PDF 1.7', 'title': 'Beyond the Loss Curve: Scaling Laws, Active Learning, and the Limits of Learning from Exact Posteriors', 'author': 'Arian Khorasani; Nathaniel Chen; Yug D Oswal; Akshat Santhana Gopalan; Egemen Kolemen; Ravid Shwartz-Ziv', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 3}, page_content='Decomposing Neural Network Error via Flow-Based Oracles\\n2017). Access to the true posterior p(y|x) enables a decom-\\nposition that resolves both questions. Consider a classifier\\nqθ(y|x) and measure its expected cross-entropy:\\nL(qθ) = Ex\\n\"\\n−\\nX\\ny\\np(y|x) log qθ(y|x)\\n#\\n(3)\\nThis loss splits cleanly into two terms:\\nL(qθ) = Ex [H(p(y|x))]\\n|\\n{z\\n}\\naleatoric\\n+ Ex [KL(p(y|x)∥qθ(y|x))]\\n|\\n{z\\n}\\nepistemic\\n(4)\\nThe aleatoric term is the entropy of the true posterior aver-\\naged over inputs. It reflects irreducible uncertainty, or the\\nambiguity inherent in the data that no classifier can resolve.\\nAn image of a cat-like dog has high H(p(y|x)) regardless\\nof model quality.\\nThe epistemic term measures the gap between the model’s\\nbeliefs and truth (Gal & Ghahramani, 2016). A perfect\\nmodel achieves KL = 0; any deviation indicates something\\nlearnable that the model has not yet captured.\\nOn standard benchmarks, we observe only total loss. When\\nit plateaus, we cannot tell whether the model has reached the\\naleatoric floor or whether epistemic error remains. Our ora-\\ncle makes both terms measurable. This distinction matters:\\nif epistemic error dominates, more data or capacity should\\nhelp; if aleatoric error dominates, improvements require\\nchanging the task itself.\\n4. Experiments\\nSetup.\\nWe trained TarFlow (Zhai et al., 2025) on\\nAFHQ (Choi et al., 2020) (3 classes: cat, dog, wild; ∼15K\\nimages) and ImageNet (Deng et al., 2009) at 64×64 res-\\nolution. Oracle quality is validated in Section 2.3. We\\ngenerated labeled samples with exact posteriors for all exper-\\niments. We trained classifiers (ResNet-50 (He et al., 2015),\\nViT-Base (Dosovitskiy et al., 2021), ConvNeXt (Liu et al.,\\n2022), Swin (Liu et al., 2021), MobileNetV3 (Howard et al.,\\n2019)) using default hyperparameters on varying amounts of\\noracle data, from N=100 to N=10,000 (up to N=45, 000\\nfor ImageNet-64) samples. All results are averaged over 3\\nseeds with standard deviation shown.\\n4.1. Scaling Laws Beyond the Loss Curve\\nBottom line: Epistemic error follows a clean power law\\nin dataset size, continuing to shrink even when total loss\\nplateaus. Standard metrics hide this ongoing learning.\\nWe trained classifiers on varying amounts of oracle\\ndata and decomposed prediction error into aleatoric\\nand epistemic components (Kaplan et al., 2020) (Sec-\\ntion 3). For each model, we computed total cross-entropy\\nloss (what standard benchmarks measure), the aleatoric\\ncomponent E[H(p(y|x))], and the epistemic component\\nE[KL(p(y|x)∥qθ(y|x))].\\nFigure 2 shows the result. Total cross-entropy loss decreases\\nwith dataset size on a log-log scale, consistent with Kaplan\\net al. (2020). MobileNet exhibits the highest loss throughout,\\nResNet and ConvNeXt the lowest. But the total loss curve\\nobscures what is actually happening.\\nThe epistemic component behaves differently. On a log-log\\nplot, it traces a nearly perfect straight line: KL ∝N −α\\n(dashed lines in Figure 2b) (Hoffmann et al., 2022). This\\ndecay continues even when total loss appears to plateau,\\nbecause the aleatoric floor dominates the total at large N.\\nStandard metrics miss this ongoing learning because they\\nconflate the two error sources.\\nThe scaling exponents α differ across architectures (Fig-\\nure 2d): ResNet-50 (α=0.039 ± 0.007), ViT-Base (0.032 ±\\n0.010), ConvNeXt (0.030 ± 0.008), Swin (0.030 ± 0.005),\\nand MobileNetV3 (0.135 ± 0.025). These quantify how\\nefficiently each architecture converts additional data into\\nreduced uncertainty; MobileNet’s much higher α indicates\\nfaster epistemic decay rate, yet it starts from a higher error\\nfloor, suggesting a less efficient initial representation. Cali-\\nbration error (ECE) also decreases with N, but at a different\\nrate than epistemic error, confirming that calibration and pos-\\nterior approximation quality are distinct phenomena (Guo\\net al., 2017).\\nThe aleatoric floor.\\nOn AFHQ, the aleatoric uncertainty\\nis exactly 0.012 nats (Bayes error 0.23%; see Section 2.3),\\nwith mutual information I(X; Y ) = 1.577 bits (normalized\\nMI = 99.5%). On ImageNet-64, the aleatoric floor is 0.10\\nnats (Bayes error 4.031%), higher due to the greater number\\nof classes and inherent inter-class ambiguity.\\nImageNet replication.\\nFigure 6 shows scaling laws on\\nImageNet-64 up to 45K samples, a substantially more chal-\\nlenging dataset than AFHQ due to its fine-grained structure\\nand 1000-way label space. Despite this higher intrinsic en-\\ntropy, the total cross-entropy loss decreases rapidly with\\ndataset size, at a faster absolute rate than in AFHQ, rein-\\nforcing the classical finding that larger datasets continue to\\nyield measurable gains even when the absolute loss remains\\ndominated by intrinsic class complexity.\\nThe aleatoric component remains effectively constant at\\napproximately 0.10 nats across all dataset sizes, confirming\\nthat all models converge to the same irreducible uncertainty\\ninduced by the oracle conditional distribution. As in AFHQ,\\nthis floor is architecture-independent and invariant to dataset\\nsize, reflecting the Bayes error of the oracle world rather\\nthan model capacity. The higher value relative to AFHQ\\n(0.012 nats) is consistent with ImageNet-64’s greater inter-\\n4'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:57610bf)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'total_pages': 19, 'format': 'PDF 1.7', 'title': 'Beyond the Loss Curve: Scaling Laws, Active Learning, and the Limits of Learning from Exact Posteriors', 'author': 'Arian Khorasani; Nathaniel Chen; Yug D Oswal; Akshat Santhana Gopalan; Egemen Kolemen; Ravid Shwartz-Ziv', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 4}, page_content='Decomposing Neural Network Error via Flow-Based Oracles\\nFigure 2. Epistemic error follows a power law even when total loss plateaus. (a) Total cross-entropy decreases with dataset size;\\nMobileNet shows the highest loss, ResNet/ConvNeXt the lowest. (b) Epistemic uncertainty (KL from oracle) follows N −α; dashed\\nlines are power-law fits. This decay continues even when total loss appears flat. (c) Accuracy improves from 92–96% at N=100 toward\\n97–98% at N=10,000. (d) Scaling exponents reveal architectural differences: MobileNet improves fastest (α=0.135), ViT stalls without\\npretraining (α=0.032).\\nclass overlap and multimodality.\\nEpistemic uncertainty decays with dataset size and cor-\\nroborates the power-law behaviour observed on AFHQ.\\nImageNet-64, however, exhibits a brief plateau in the mid-\\nrange (2,000-10,000 samples), reflecting a representation-\\ntransition regime in which models do not immediately con-\\nvert additional data into reduced epistemic uncertainty. Be-\\nyond this regime, all architectures resume clear power-law\\ndecay, reinforcing that epistemic uncertainty, alongside total\\nloss, is a distinct and reliable indicator of continued learning\\nat scale.\\nThe resulting exponents show both similarities and diver-\\ngences relative to AFHQ. ResNet-50 exhibits the highest\\nscaling rate (α=0.019 ± 0.006), closely followed by ViT-\\nBase (0.018 ± 0.004), mirroring their behaviour on AFHQ.\\nConvNeXt achieves only a weak decay rate (0.003 ± 0.003,\\nlower than ResNet and ViT-Base as in AFHQ), indicating\\nlimited epistemic improvement per additional sample, likely\\ndue to architectural biases that mismatch the ImageNet-64\\nfeature geometry. Swin-T displays the strongest positive\\nexponent (0.027 ± 0.01), consistent with hierarchical trans-\\nformers benefiting disproportionately from larger, more di-\\nverse datasets.\\nMobileNet-V3 is an exception: its epistemic curve exhibits\\na pronounced non-monotonicity. We attribute this to Mo-\\nbileNet’s compressed architecture and depthwise-separable\\nconvolutions, which yield an inefficient initial representa-\\ntion incapable of faithfully capturing ImageNet-64’s high\\ninter-class diversity. At small dataset sizes, the model over-\\nfits and becomes overconfident, producing an artificially\\nlow epistemic floor. As the dataset expands into the mid-\\nscale regime (≈2k–10k samples), this brittle representation\\nis forced to reorganize, triggering a spike in loss, calibra-\\ntion error, and epistemic uncertainty. Once past this regime,\\nMobileNet resumes clean power-law decay, confirming that\\nthe underlying epistemic scaling law still holds once an\\nadequate representation is established.\\nTogether with the monotonic rise in test accuracy, these\\ntrends show that accuracy, calibration, and epistemic uncer-\\ntainty respond differently to data scaling, with calibration\\nshowing partial signatures of the behaviour made explicit\\nby epistemic uncertainty.\\nArchitectures differ in how they approach optimality.\\nTable 2 shows that ResNet-18 exhibits clean power-law\\nscaling: epistemic error drops from 0.16 to 0.026 as data\\nincreases from N=100 to N=40,000 (a 6× reduction). ViT-\\nBase behaves differently: it starts competitive at N=100 but\\n5'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:57610bf)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'total_pages': 19, 'format': 'PDF 1.7', 'title': 'Beyond the Loss Curve: Scaling Laws, Active Learning, and the Limits of Learning from Exact Posteriors', 'author': 'Arian Khorasani; Nathaniel Chen; Yug D Oswal; Akshat Santhana Gopalan; Egemen Kolemen; Ravid Shwartz-Ziv', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 5}, page_content='Decomposing Neural Network Error via Flow-Based Oracles\\nTable 2. ResNets reduce epistemic error 6×; ViTs stall with-\\nout pretraining. Epistemic KL (nats) vs. dataset size on AFHQ.\\nResNet exhibits clean power-law scaling from 0.16 to 0.026; ViT\\nimproves by only 1.2× over the same range.\\nDataset size N\\nResNet-18\\nViT-B-16\\n100\\n0.160\\n0.131\\n1,000\\n0.090\\n0.141\\n5,000\\n0.058\\n0.099\\n10,000\\n0.033\\n0.117\\n40,000\\n0.026\\n0.117\\nthen stalls. By N=40,000, ViT has improved by less than\\n1.2×. Both architectures achieve similar total loss at large\\nN; the difference is in how they approach optimality. ViTs,\\nlacking pretraining, fail to extract geometric structure from\\nsmall samples (Dosovitskiy et al., 2021). This is consistent\\nwith Dosovitskiy et al. (2021), but our framework quantifies\\nthe gap in information-theoretic terms.\\n4.2. Soft Labels Enable Better Learning\\nOur oracle can generate exact posterior distributions p(y|x)\\nas training labels, not just the argmax class. Training with\\nthese soft labels outperforms hard-label training at 4 out of\\n5 dataset sizes, with accuracy gains up to ∼1%. Models\\ntrained on soft labels also achieve near-perfect calibration\\n(ECE = 0.018) (Guo et al., 2017), confirming the posteriors\\nencode learnable structure beyond class labels (Szegedy\\net al., 2015; Hinton et al., 2015). Full results in Appendix D.\\n4.3. Distribution Shift: Exact Quantification via Oracle\\nBottom line: The oracle computes exact KL divergence\\nof controlled perturbations. Class imbalance barely af-\\nfects accuracy at KL values where input noise causes\\ncollapse. Shift type matters far more than shift magni-\\ntude.\\nOn standard benchmarks, distribution shift is observed but\\nnever measured: we see accuracy drop but cannot quantify\\nhow much the distribution actually changed. Our oracle\\ncomputes KL[pshifted∥pbaseline] exactly by evaluating the true\\nlog-likelihood log p(x|y) under both distributions. This\\nenables controlled experiments that are impossible on real\\nbenchmarks: we introduce perturbations of known type and\\nmagnitude, measure the exact divergence, and observe how\\nmodels degrade.\\nWe introduce two types of controlled perturbation to the\\noracle distribution:\\n• Class imbalance: Shifting the class prior from uni-\\nform to skewed ratios (e.g., 40/35/25 up to 70/20/10),\\nwhich increases KL divergence through the marginal\\np(y) (Chawla et al., 2002).\\n• Gaussian noise: Adding noise with standard deviation\\nσ ∈{0.05, 0.10, 0.15} to generated images, which in-\\ncreases KL divergence through the conditional p(x|y).\\nThe results (Figure 3) show a clear asymmetry. Class im-\\nbalance produces KL divergences from 0.018 to 0.293, yet\\naccuracy remains flat at 97.4–97.7%. Gaussian noise at\\nσ=0.15 produces a comparable KL divergence but causes\\naccuracy to collapse to ∼77%, a 20-percentage-point drop.\\nThe same KL magnitude leads to dramatically different\\noutcomes depending on where the shift occurs.\\nClass imbalance shifts the marginal p(y) but leaves the\\nclass-conditional p(x|y) intact, so learned features remain\\ndiscriminative. Gaussian noise corrupts p(x|y) directly, de-\\nstroying the features that classifiers depend on (Hendrycks\\n& Dietterich, 2019). A linear fit of KL divergence against\\naccuracy drop yields R2 = 0.042 (Figure 3f): aggregate di-\\nvergence does not predict robustness. What shifted matters\\nmore than how much.\\n4.4. Active Learning with Ground-Truth Uncertainty\\nActive learning aims to select the most informative training\\nsamples (Sener & Savarese, 2018). On standard bench-\\nmarks, informativeness must be estimated via heuristics\\n(e.g., entropy of model predictions) (Lewis & Gale, 1994;\\nGal et al., 2017). Our oracle provides exact epistemic un-\\ncertainty, enabling a critical distinction: separating samples\\nthat are epistemically uncertain (informative) from those\\nthat are aleatorically uncertain (inherently ambiguous). On\\nstandard benchmarks, a high-entropy sample could be either;\\nthe oracle tells us which.\\nWe compare three acquisition strategies on ImageNet-64:\\nRandom, Max Entropy, and Max Epistemic (selecting by\\nexact epistemic uncertainty). Max Epistemic achieves a\\n1.6× larger accuracy gain than Max Entropy under the same\\nlabeling budget, and requires 47.8% fewer labeled queries\\nto reach the same mid-regime performance. Max Epistemic\\nconsistently outperforms both baselines, confirming that\\npredictive entropy is a poor proxy for information gain when\\naleatoric noise is substantial. Full results and learning curves\\nappear in Appendix (Figure 9).\\n6'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:57610bf)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'total_pages': 19, 'format': 'PDF 1.7', 'title': 'Beyond the Loss Curve: Scaling Laws, Active Learning, and the Limits of Learning from Exact Posteriors', 'author': 'Arian Khorasani; Nathaniel Chen; Yug D Oswal; Akshat Santhana Gopalan; Egemen Kolemen; Ravid Shwartz-Ziv', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 6}, page_content='Decomposing Neural Network Error via Flow-Based Oracles\\nFigure 3. What shifts matters more than how much: KL magnitude alone poorly predicts performance (R2=0.04). (a) Test accuracy\\nvs. exact KL divergence: class imbalance (blue) maintains ∼97% accuracy; Gaussian noise (low points) collapses to ∼77% at comparable\\nKL. (b) Accuracy drop vs. KL: imbalance (red diamonds) clusters near zero regardless of divergence magnitude. (c) Prior shift alone\\nbarely affects accuracy (97.4–97.7% across all imbalance levels). (d) Covariate shift causes exponential degradation (σ=0.15 →77%).\\n(e) Aggregated comparison confirms noise dominates. (f) Linear regression: R2 = 0.04 shows aggregate KL is uninformative.\\n5. Discussion\\nImplications for practice.\\nFirst, validation loss is a poor\\nproxy for learning progress: epistemic error continues to\\nshrink long after loss curves flatten. Practitioners using\\nearly stopping may be leaving performance on the table.\\nTracking epistemic proxies (e.g., ensemble disagreement) is\\na better strategy, though in real settings these must be esti-\\nmated (Lakshminarayanan et al., 2017; Gal & Ghahramani,\\n2016).\\nSecond, in small-data regimes without pretraining, convo-\\nlutional architectures are the better choice: ResNets reduce\\nepistemic error by 6× from N=100 to N=40K, while ViTs\\nimprove by less than 1.2×.\\nThird, the soft label results suggest that access to posterior\\ninformation, even approximate, can meaningfully improve\\nclassification. This connects to knowledge distillation (Hin-\\nton et al., 2015): a strong teacher’s soft outputs contain struc-\\nture beyond hard labels. Our oracle provides a controlled\\nsetting to study this phenomenon with exact posteriors.\\nFourth, a shift in p(x|y) (feature corruption) is far more\\ndamaging than a shift in p(y) (prior change) (Hendrycks\\n& Dietterich, 2019; Chawla et al., 2002) at comparable KL\\nvalues. Robustness evaluations should characterize what\\nshifted, not just how much.\\nLimitations.\\nOur oracle defines a synthetic world, not\\nnature. Findings transfer to natural images only insofar as\\nthe flow captures relevant statistical structure. Key concerns:\\n(1) domain gap between flow samples and real photographs;\\n(2) scale limitations (AFHQ has 3 classes; full ImageNet\\nexperiments ongoing); (3) architectural conclusions may\\ndepend on hyperparameters. We view this framework as\\ncomplementary to standard benchmarks: it provides ground\\ntruth within a controlled setting, not claims about nature’s\\ntrue Bayes error.\\n6. Related Work\\nNeural scaling laws.\\nKaplan et al. (2020) established that\\nloss decreases as a power law in data, compute, and parame-\\nters. Subsequent work refined these laws for vision (Zhai\\net al., 2022) and explored their limits. All such studies mea-\\nsure total loss, conflating aleatoric and epistemic error. Our\\ndecomposition reveals that epistemic error alone follows a\\npower law, even when total loss appears flat.\\nUncertainty estimation.\\nBayesian deep learning and en-\\nsemble methods aim to quantify epistemic uncertainty, but\\nwithout ground truth, evaluation is indirect (Alemi et al.,\\n7'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:57610bf)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'total_pages': 19, 'format': 'PDF 1.7', 'title': 'Beyond the Loss Curve: Scaling Laws, Active Learning, and the Limits of Learning from Exact Posteriors', 'author': 'Arian Khorasani; Nathaniel Chen; Yug D Oswal; Akshat Santhana Gopalan; Egemen Kolemen; Ravid Shwartz-Ziv', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 7}, page_content='Decomposing Neural Network Error via Flow-Based Oracles\\n2019). Calibration metrics compare to empirical frequen-\\ncies, not true posteriors (Guo et al., 2017). Our framework\\nprovides the missing ground truth, enabling evaluation of\\nboth calibration and posterior quality.\\nKnowledge distillation and soft labels.\\nHinton et al.\\n(2015) showed that training on a teacher’s soft predictions\\ntransfers “dark knowledge” beyond hard labels. Our soft-\\nlabel experiments connect to this literature but with a key\\ndifference: our soft labels are exact posteriors, not approx-\\nimations from a teacher model. This provides an upper\\nbound on what soft-label training can achieve.\\nDistribution\\nshift.\\nBenchmarks\\nlike\\nImageNet-\\nC (Hendrycks & Dietterich, 2019) evaluate robustness to\\ndistribution shift, but the magnitude of shift is unknown.\\nOur oracle enables controlled experiments with exact\\ndivergence, revealing that shift type matters more than\\nmagnitude.\\nActive learning.\\nUncertainty-based acquisition func-\\ntions (Gal et al., 2017; Kirsch et al., 2019) select samples by\\nestimated informativeness. Our oracle separates epistemic\\nfrom aleatoric uncertainty exactly, providing an upper bound\\non what uncertainty-based active learning can achieve.\\nNormalizing flows.\\nFlows have matured from Real-\\nNVP (Dinh et al., 2017) and Glow (Kingma & Dhariwal,\\n2018) to TarFlow (Zhai et al., 2025), which achieves strong\\nlikelihoods on ImageNet. We use flows differently: not\\nas models to evaluate, but as oracles that define evaluation\\nitself.\\nSynthetic benchmarks.\\nResearchers have long used toy\\ndistributions (Gaussians, moons, spirals) with known poste-\\nriors. These lack realism. Our approach combines realistic\\nimages from AFHQ and ImageNet with the exact posterior\\naccess of synthetic benchmarks.\\n7. Conclusion\\nWe introduced a framework that treats normalizing flows\\nas oracles rather than models, enabling exact computation\\nof quantities that standard benchmarks can only estimate.\\nWhen we know the data-generating process, we can directly\\nmeasure what classifiers learn versus what remains funda-\\nmentally uncertain.\\nOur experiments reveal that epistemic error follows a clean\\npower law (KL ∝N −α) even when total loss plateaus:\\nmodels keep learning, but standard metrics miss it. This\\npower-law decay holds across architectures and scales to Im-\\nageNet with 1000 classes. The decomposition also reveals\\nmarked architectural differences: ResNets reduce epistemic\\nerror by 6× over the data range where ViTs improve by\\nonly 1.2×, suggesting convolutional inductive biases matter\\nmost in low-data regimes.\\nOur distribution shift experiments show that what shifts\\nmatters more than how much: class imbalance barely\\naffects accuracy at KL values where input noise causes\\n20-point drops. This finding implies that robustness eval-\\nuations should characterize the nature of shift, not just its\\nmagnitude.\\nThe oracle framework is not a replacement for real-world\\nbenchmarks; it complements them by providing ground\\ntruth within a controlled setting. This approach can inform\\narchitecture selection, training decisions, and robustness\\nevaluation in ways that aggregate loss curves cannot. Code\\nand oracle models will be made available soon.\\nImpact Statement\\nThis paper presents work whose goal is to advance the field\\nof Machine Learning by providing benchmarks with ex-\\nact information-theoretic ground truth.\\nOur framework\\nenables more rigorous evaluation of learning algorithms,\\nwhich should lead to better scientific understanding of deep\\nlearning. The oracle datasets we generate are synthetic and\\ndo not raise privacy concerns. There are many potential so-\\ncietal consequences of our work, none which we feel must\\nbe specifically highlighted here.\\nAcknowledgments\\nThis research was supported in part by the Digital Research\\nAlliance of Canada (DRAC), Calcul Qu´ebec, and the Prince-\\nton Laboratory for Artificial Intelligence, under Award No.\\n2025-97. We also acknowledge Vellore Institute of Tech-\\nnology (VIT) for providing access to eight NVIDIA V100\\nGPUs, which supported the computational experiments re-\\nported in this work.\\n8. Contributions\\nArian organized, planned, and led the project; developed\\nand implemented the core idea and codebase; refined the\\narchitecture by improving key components and training\\nstrategies; iterated on the dataset; and ran and iterated on all\\nexperiments.\\nNathaniel integrated all the experiments for the ImageNet-\\n128 dataset and contributed meaningfully to writing the\\npaper.\\nYug integrated all the experiments for the ImageNet-64\\ndataset and contributed meaningfully to writing the paper.\\nAkshat contributed to the writing of the paper.\\n8'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:57610bf)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'total_pages': 19, 'format': 'PDF 1.7', 'title': 'Beyond the Loss Curve: Scaling Laws, Active Learning, and the Limits of Learning from Exact Posteriors', 'author': 'Arian Khorasani; Nathaniel Chen; Yug D Oswal; Akshat Santhana Gopalan; Egemen Kolemen; Ravid Shwartz-Ziv', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 8}, page_content='Decomposing Neural Network Error via Flow-Based Oracles\\nEgemen Advised the project in high-level.\\nRavid advised the whole project from scratch, with feed-\\nback in all the stages, contributing to the writing of the paper,\\nand conceiving and pushing forward the research direction\\nin the all stages of the project.\\nReferences\\nAlemi, A. A., Fischer, I., Dillon, J. V., and Murphy,\\nK.\\nDeep Variational Information Bottleneck, Octo-\\nber 2019.\\nURL http://arxiv.org/abs/1612.\\n00410. arXiv:1612.00410 [cs].\\nBelghazi, M. I., Baratin, A., Rajeswar, S., Ozair, S., Bengio,\\nY., Courville, A., and Hjelm, R. D. MINE: Mutual Infor-\\nmation Neural Estimation, August 2021. URL http://\\narxiv.org/abs/1801.04062. arXiv:1801.04062\\n[cs].\\nChawla, N. V., Bowyer, K. W., Hall, L. O., and Kegelmeyer,\\nW. P. SMOTE: Synthetic Minority Over-sampling Tech-\\nnique. Journal of Artificial Intelligence Research, 16:\\n321–357, June 2002. ISSN 1076-9757. doi: 10.1613/jair.\\n953. URL http://arxiv.org/abs/1106.1813.\\narXiv:1106.1813 [cs].\\nChoi, Y., Uh, Y., Yoo, J., and Ha, J.-W.\\nStarGAN\\nv2: Diverse Image Synthesis for Multiple Domains,\\nApril 2020. URL http://arxiv.org/abs/1912.\\n01865. arXiv:1912.01865 [cs].\\nChrabaszcz, P., Loshchilov, I., and Hutter, F. A Downsam-\\npled Variant of ImageNet as an Alternative to the CIFAR\\ndatasets, August 2017. URL http://arxiv.org/\\nabs/1707.08819. arXiv:1707.08819 [cs].\\nDeng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei,\\nL. ImageNet: A large-scale hierarchical image database.\\nIn 2009 IEEE Conference on Computer Vision and Pat-\\ntern Recognition, pp. 248–255, June 2009. doi: 10.1109/\\nCVPR.2009.5206848. URL https://ieeexplore.\\nieee.org/document/5206848.\\nISSN: 1063-\\n6919.\\nDinh, L., Sohl-Dickstein, J., and Bengio, S. Density esti-\\nmation using Real NVP, February 2017. URL http://\\narxiv.org/abs/1605.08803. arXiv:1605.08803\\n[cs].\\nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn,\\nD., Zhai, X., Unterthiner, T., Dehghani, M., Minderer,\\nM., Heigold, G., Gelly, S., Uszkoreit, J., and Houlsby,\\nN. An Image is Worth 16x16 Words: Transformers for\\nImage Recognition at Scale, June 2021. URL http://\\narxiv.org/abs/2010.11929. arXiv:2010.11929\\n[cs].\\nGal, Y. and Ghahramani, Z. Dropout as a Bayesian Approx-\\nimation: Representing Model Uncertainty in Deep Learn-\\ning. In Proceedings of The 33rd International Conference\\non Machine Learning, pp. 1050–1059. PMLR, June 2016.\\nURL https://proceedings.mlr.press/v48/\\ngal16.html.\\nGal,\\nY.,\\nIslam,\\nR.,\\nand Ghahramani,\\nZ.\\nDeep\\nBayesian Active Learning with Image Data, March\\n2017.\\nURL\\nhttp://arxiv.org/abs/1703.\\n02910. arXiv:1703.02910 [cs].\\nGuo, C., Pleiss, G., Sun, Y., and Weinberger, K. Q.\\nOn Calibration of Modern Neural Networks, Au-\\ngust 2017. URL http://arxiv.org/abs/1706.\\n04599. arXiv:1706.04599 [cs].\\nHe, K., Zhang, X., Ren, S., and Sun, J.\\nDeep\\nResidual Learning for Image Recognition, Decem-\\nber 2015.\\nURL http://arxiv.org/abs/1512.\\n03385. arXiv:1512.03385 [cs].\\nHendrycks, D. and Dietterich, T. Benchmarking Neural\\nNetwork Robustness to Common Corruptions and Per-\\nturbations, March 2019. URL http://arxiv.org/\\nabs/1903.12261. arXiv:1903.12261 [cs].\\nHeusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., and\\nHochreiter, S. GANs Trained by a Two Time-Scale Up-\\ndate Rule Converge to a Local Nash Equilibrium, Jan-\\nuary 2018. URL http://arxiv.org/abs/1706.\\n08500. arXiv:1706.08500 [cs].\\nHinton, G., Vinyals, O., and Dean, J. Distilling the Knowl-\\nedge in a Neural Network, March 2015. URL http://\\narxiv.org/abs/1503.02531. arXiv:1503.02531\\n[stat].\\nHoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E.,\\nCai, T., Rutherford, E., Casas, D. d. L., Hendricks, L. A.,\\nWelbl, J., Clark, A., Hennigan, T., Noland, E., Millican,\\nK., Driessche, G. v. d., Damoc, B., Guy, A., Osindero,\\nS., Simonyan, K., Elsen, E., Rae, J. W., Vinyals, O., and\\nSifre, L. Training Compute-Optimal Large Language\\nModels, March 2022.\\nURL http://arxiv.org/\\nabs/2203.15556. arXiv:2203.15556 [cs].\\nHoward, A., Sandler, M., Chu, G., Chen, L.-C., Chen, B.,\\nTan, M., Wang, W., Zhu, Y., Pang, R., Vasudevan, V.,\\nLe, Q. V., and Adam, H. Searching for MobileNetV3,\\nNovember 2019. URL http://arxiv.org/abs/\\n1905.02244. arXiv:1905.02244 [cs].\\nKaplan, J., McCandlish, S., Henighan, T., Brown, T. B.,\\nChess, B., Child, R., Gray, S., Radford, A., Wu, J., and\\nAmodei, D. Scaling Laws for Neural Language Mod-\\nels, January 2020. URL http://arxiv.org/abs/\\n2001.08361. arXiv:2001.08361 [cs].\\n9'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:57610bf)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'total_pages': 19, 'format': 'PDF 1.7', 'title': 'Beyond the Loss Curve: Scaling Laws, Active Learning, and the Limits of Learning from Exact Posteriors', 'author': 'Arian Khorasani; Nathaniel Chen; Yug D Oswal; Akshat Santhana Gopalan; Egemen Kolemen; Ravid Shwartz-Ziv', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 9}, page_content='Decomposing Neural Network Error via Flow-Based Oracles\\nKingma, D. P. and Dhariwal, P. Glow: Generative Flow with\\nInvertible 1x1 Convolutions, July 2018. URL http://\\narxiv.org/abs/1807.03039. arXiv:1807.03039\\n[stat].\\nKirsch, A., Amersfoort, J. v., and Gal, Y. BatchBALD: Effi-\\ncient and Diverse Batch Acquisition for Deep Bayesian\\nActive Learning, October 2019. URL http://arxiv.\\norg/abs/1906.08158. arXiv:1906.08158 [cs].\\nLakshminarayanan, B., Pritzel, A., and Blundell, C. Sim-\\nple and Scalable Predictive Uncertainty Estimation us-\\ning Deep Ensembles, November 2017. URL http://\\narxiv.org/abs/1612.01474. arXiv:1612.01474\\n[stat].\\nLewis, D. D. and Gale, W. A. A Sequential Algorithm for\\nTraining Text Classifiers, July 1994. URL http://\\narxiv.org/abs/cmp-lg/9407020.\\narXiv:cmp-\\nlg/9407020.\\nLiu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z.,\\nLin, S., and Guo, B.\\nSwin Transformer: Hierarchi-\\ncal Vision Transformer using Shifted Windows, Au-\\ngust 2021. URL http://arxiv.org/abs/2103.\\n14030. arXiv:2103.14030 [cs].\\nLiu, Z., Mao, H., Wu, C.-Y., Feichtenhofer, C., Dar-\\nrell, T., and Xie, S.\\nA ConvNet for the 2020s,\\nMarch 2022.\\nURL http://arxiv.org/abs/\\n2201.03545. arXiv:2201.03545 [cs].\\nPapamakarios, G., Nalisnick, E., Rezende, D. J., Mo-\\nhamed, S., and Lakshminarayanan, B.\\nNormaliz-\\ning Flows for Probabilistic Modeling and Inference,\\nApril 2021. URL http://arxiv.org/abs/1912.\\n02762. arXiv:1912.02762 [stat].\\nSalimans, T., Goodfellow, I., Zaremba, W., Cheung, V., Rad-\\nford, A., and Chen, X. Improved Techniques for Training\\nGANs, June 2016. URL http://arxiv.org/abs/\\n1606.03498. arXiv:1606.03498 [cs].\\nSener, O. and Savarese, S.\\nActive Learning for Con-\\nvolutional Neural Networks:\\nA Core-Set Approach,\\nJune 2018. URL http://arxiv.org/abs/1708.\\n00489. arXiv:1708.00489 [stat].\\nSzegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., and Wojna,\\nZ. Rethinking the Inception Architecture for Computer\\nVision, December 2015. URL http://arxiv.org/\\nabs/1512.00567. arXiv:1512.00567 [cs].\\nZhai, S., Zhang, R., Nakkiran, P., Berthelot, D., Gu, J.,\\nZheng, H., Chen, T., Bautista, M. A., Jaitly, N., and\\nSusskind, J. Normalizing Flows are Capable Generative\\nModels, June 2025. URL http://arxiv.org/abs/\\n2412.06329. arXiv:2412.06329 [cs].\\nZhai, X., Kolesnikov, A., Houlsby, N., and Beyer, L. Scaling\\nVision Transformers, June 2022. URL http://arxiv.\\norg/abs/2106.04560. arXiv:2106.04560 [cs].\\n10'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:57610bf)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'total_pages': 19, 'format': 'PDF 1.7', 'title': 'Beyond the Loss Curve: Scaling Laws, Active Learning, and the Limits of Learning from Exact Posteriors', 'author': 'Arian Khorasani; Nathaniel Chen; Yug D Oswal; Akshat Santhana Gopalan; Egemen Kolemen; Ravid Shwartz-Ziv', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 10}, page_content='Decomposing Neural Network Error via Flow-Based Oracles\\nA. Oracle Validation Details\\nWe validated the oracle along five axes on both AFHQ and ImageNet-64. This appendix provides full details for the summary\\nin Section 2. We report quantitative checks verifying our Oracle provides (i) high-quality samples, (ii) broad support over\\nthe data manifold, and (iii) stable posteriors for uncertainty decomposition.\\nA.1. Distribution Quality (FID)\\nWe measured Fr´echet Inception Distance (FID) (Heusel et al., 2018), which compares the mean and covariance of Inception-\\nv3 features between real and generated images. Our AFHQ oracle achieves FID 28.44 and ImageNet-64 achieves FID 13.48,\\ncomparable to state-of-the-art generative models on these datasets.\\nA.2. Manifold Coverage\\nA generative model might produce high-quality samples that only capture part of the true distribution (e.g., generating\\nrealistic dogs but missing rare breeds). We computed manifold coverage by embedding both real and generated samples in a\\npretrained feature space and measuring what fraction of real samples have a nearby generated sample within a distance\\nthreshold. In our implementation, distances are computed in ResNet feature space using k-nearest neighbors and an adaptive\\nthreshold (set to the 90th percentile of real-to-synthetic distances). Our oracle achieves 90% coverage on AFHQ and 89.9%\\non ImageNet, suggesting it captures broad distributional structure rather than collapsing to a few high-density modes.\\nA.3. Diversity (Inception Score)\\nWe evaluated diversity using Inception Score (IS), which measures the KL divergence between the conditional label distribu-\\ntion p(y|x) and the marginal p(y) averaged over generated samples. Higher scores indicate both confident classifications\\nand diverse outputs. We achieve 6.24 ± 3.57 on AFHQ and 32.57 ± 4.47 on ImageNet. Per-class diversity ratios (variance\\nratio and distance ratio between synthetic and real) range from 0.75 to 0.96 on AFHQ, confirming reasonable diversity that\\nis slightly below real data. Across the three classes, the synthetic-to-real feature variance ratios are {0.746, 0.854, 0.921}\\nand the average pairwise distance ratios are {0.891, 0.944, 0.965}, indicating slightly reduced diversity relative to real data,\\nbut without severe collapse\\nA.4. Semantic Feature Alignment\\nPixel-level metrics do not capture whether images have the right semantic content. We extracted features from a pretrained\\nResNet, which encode high-level structure like shape and pose, and compared statistics between real and generated images.\\nFeature variance matches at 83–92% across classes on AFHQ, with Class 0 (cat) performing best (92%) and Classes 1–2\\n(dog, wild) showing larger distributional separation but still acceptable overlap. ImageNet-64 results show 72–93% feature\\nvariance match across classes.\\nA.5. Memorization Check\\nIf the flow memorizes training images, our benchmark would be meaningless. We computed nearest-neighbor distances\\nin Inception feature space between each generated image and the training set. 36% of generated samples on AFHQ have\\na training neighbor within distance 10 in feature space; 6.53% on ImageNet. This thresholded “near-neighbor” rate is a\\nconservative proxy for potential memorization rather than direct evidence of exact duplication. Visual inspection confirms\\nthat “close” pairs share high-level attributes (pose, lighting) but depict different individuals. The nearest-neighbor distance\\ndistribution is well-separated from zero, with the memorization threshold clearly above the bulk of the distribution; (See\\nFigure 4).\\nA.6. Texture Analysis\\nGenerated images are slightly smoother than real photographs: pixel-level texture variance is 56–69% of real images across\\nall classes. This is a known property of flow-based models, which tend to soften fine-grained details while preserving global\\nstructure. For our purposes, this is acceptable because our downstream classifiers primarily rely on semantic cues (shape,\\npose, object parts) rather than fine-grained pixel texture. Since this smoothing effect is systematic across the oracle-generated\\ndataset, comparative analyses across architectures remain valid.\\n11'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:57610bf)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'total_pages': 19, 'format': 'PDF 1.7', 'title': 'Beyond the Loss Curve: Scaling Laws, Active Learning, and the Limits of Learning from Exact Posteriors', 'author': 'Arian Khorasani; Nathaniel Chen; Yug D Oswal; Akshat Santhana Gopalan; Egemen Kolemen; Ravid Shwartz-Ziv', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 11}, page_content='Decomposing Neural Network Error via Flow-Based Oracles\\nDataset\\nFID↓\\nIS↑\\nCoverage↑\\nMem. rate\\nNN dist. (med)\\nPost. stab. KL↓\\nAFHQ-256\\n28.44\\n6.24 ± 3.57\\n0.900\\n0.363\\n11.01\\n1.51 × 10−9\\nTable 3. Oracle passes all validation checks: high coverage, low memorization, stable posteriors. FID measures distributional\\nquality; coverage confirms broad manifold support (90%); memorization rate shows most samples are novel; posterior stability KL (10−9)\\nconfirms robustness to small perturbations.\\nTable 4. Oracle samples match real data statistics across multiple metrics. FID, Inception Score, and manifold coverage confirm\\ndistributional quality; low memorization rates verify sample novelty. Dashes indicate metrics not yet computed.\\nMetric\\nAFHQ\\nImageNet-64\\nFID ↓\\n28.44\\n13.48\\nInception Score ↑\\n6.24 ± 3.57\\n32.57 ± 4.47\\nManifold coverage\\n90%\\n89.9%\\nFeature variance match\\n83–92%\\n72–93%\\nTexture variance ratio\\n56–69%\\n—\\nFeature variance ratio match\\n—\\n71.6–93.3%\\nMemorization rate\\n36%\\n6.5%\\nFigure 4. Oracle samples are not memorized from training data. Distribution of nearest-neighbor distances (ResNet feature space)\\nbetween generated and training images. The dashed line marks the memorization threshold (d=10); the bulk of distances lie well above,\\nconfirming sample novelty.\\n12'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:57610bf)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'total_pages': 19, 'format': 'PDF 1.7', 'title': 'Beyond the Loss Curve: Scaling Laws, Active Learning, and the Limits of Learning from Exact Posteriors', 'author': 'Arian Khorasani; Nathaniel Chen; Yug D Oswal; Akshat Santhana Gopalan; Egemen Kolemen; Ravid Shwartz-Ziv', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 12}, page_content='Decomposing Neural Network Error via Flow-Based Oracles\\nB. TarFlow Architecture\\nWe train TarFlow models from scratch on AFHQ at 256×256 and class-conditional ImageNet at 64×64 and 128×128,\\nfollowing the architecture introduced by Zhai et al. (2025). All configurations use an autoregressive Transformer backbone\\nwith 8 blocks and 8 flow layers per block. For all datasets, we use the same optimizer settings (learning rate 10−4) and\\nlabel-dropout of 0.1 for class-conditional training. Input dequantization is performed by adding Gaussian noise with\\ndataset-specific standard deviation σ (Table 5). We compute and cache dataset statistics required for FID evaluation using\\nthe corresponding ground-truth training distribution at each resolution prior to sampling and evaluation.\\nTraining and evaluation protocol.\\nWe train using distributed data-parallelism with dataset-dependent GPU counts (8\\nGPUs for AFHQ 2562 and ImageNet 642; 32 GPUs for ImageNet 1282). During training, we periodically generate samples\\nand evaluate FID using cached ground-truth statistics computed at the matching resolution. For conditional generation at\\nevaluation time, we additionally report classifier-free guidance (CFG) results by sampling with a nonzero guidance scale\\n(e.g., γ = 2.3 for ImageNet 642), while keeping guidance disabled during training. We emphasize that all reported ImageNet\\nand AFHQ models are trained from scratch using the above protocol.\\n13'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:57610bf)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'total_pages': 19, 'format': 'PDF 1.7', 'title': 'Beyond the Loss Curve: Scaling Laws, Active Learning, and the Limits of Learning from Exact Posteriors', 'author': 'Arian Khorasani; Nathaniel Chen; Yug D Oswal; Akshat Santhana Gopalan; Egemen Kolemen; Ravid Shwartz-Ziv', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 13}, page_content='Decomposing Neural Network Error via Flow-Based Oracles\\nHyperparameter\\nAFHQ 2562\\nImageNet 642\\nImageNet 1282\\nConditioning\\nClass-conditional\\nClass-conditional\\nClass-conditional\\nImage size\\n256\\n64\\n128\\nChannels (C)\\n768\\n768\\n1024\\nPatch size\\n8\\n2\\n4\\n# Transformer blocks\\n8\\n8\\n8\\nFlow layers / block\\n8\\n8\\n8\\nGaussian noise std (σ)\\n0.07\\n0.05\\n0.15\\nLearning rate\\n1 × 10−4\\n1 × 10−4\\n1 × 10−4\\nBatch size\\n256\\n256\\n768\\nEpochs\\n4000\\n200\\n320\\nLabel dropout (p)\\n0.1\\n0.1\\n0.1\\nCFG during training\\n0 (disabled)\\n0 (disabled)\\n0 (disabled)\\nTable 5. TarFlow hyperparameters for reproducibility. All models use 8 Transformer blocks with 8 flow layers each. Gaussian noise\\nrefers to dequantization noise during training.\\nC. Full Scaling Results\\nFigure 5. Full scaling results on AFHQ. Epistemic uncertainty follows power-law decay across all architectures, with MobileNet showing\\nthe steepest decline and ViT the shallowest.\\n14'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:57610bf)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'total_pages': 19, 'format': 'PDF 1.7', 'title': 'Beyond the Loss Curve: Scaling Laws, Active Learning, and the Limits of Learning from Exact Posteriors', 'author': 'Arian Khorasani; Nathaniel Chen; Yug D Oswal; Akshat Santhana Gopalan; Egemen Kolemen; Ravid Shwartz-Ziv', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 14}, page_content='Decomposing Neural Network Error via Flow-Based Oracles\\nFigure 6. Scaling laws extend to ImageNet-64 with 1000 classes. Power-law epistemic decay holds at scale, though MobileNet shows a\\nmid-range transition regime before resuming clean scaling.\\nD. Soft Labels: Full Results\\nOur oracle can generate exact posterior distributions p(y|x) as training labels, not just the argmax class. We compare\\ntraining with hard labels (one-hot from the most likely class) versus soft labels (the full posterior vector) across dataset sizes\\nfrom N=100 to N=5,000.\\nFigure 7 shows that training with oracle soft labels outperforms hard-label training at 4 out of 5 dataset sizes, with accuracy\\ngains up to ∼1%. The one exception is N=500, where hard labels slightly outperform.\\nThis result validates oracle quality from a different angle: the soft posteriors contain learnable information beyond the\\nclass label. If the oracle were merely assigning noisy labels, soft training would not consistently outperform. Instead, the\\nposteriors encode genuine uncertainty structure (inter-class similarities, ambiguous regions, confidence gradients) that\\nclassifiers can exploit. This form of supervision is unavailable on any standard benchmark, where labels are always hard.\\nModels trained on soft labels also achieve near-perfect calibration (ECE = 0.018), learning the full uncertainty landscape\\nrather than just decision boundaries.\\n15'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:57610bf)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'total_pages': 19, 'format': 'PDF 1.7', 'title': 'Beyond the Loss Curve: Scaling Laws, Active Learning, and the Limits of Learning from Exact Posteriors', 'author': 'Arian Khorasani; Nathaniel Chen; Yug D Oswal; Akshat Santhana Gopalan; Egemen Kolemen; Ravid Shwartz-Ziv', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 15}, page_content='Decomposing Neural Network Error via Flow-Based Oracles\\nFigure 7. Exact soft labels outperform hard labels at most dataset sizes. Top: Training curves show soft labels (blue) tracking above\\nhard labels (red) across three dataset sizes. Bottom left: Final accuracy vs. N; soft labels win at 4 of 5 sizes. Bottom right: Accuracy\\ngain reaches ∼1%, confirming oracle posteriors encode learnable structure beyond class labels.\\nE. Full Active Learning Results\\nFigure 8. Epistemic-based acquisition reduces uncertainty fastest on AFHQ. (a) Learning curves: Max Epistemic (red) initially dips\\nwhile selecting challenging samples, then recovers. (b) Epistemic KL reduction: Max Epistemic achieves the steepest decline, confirming\\nit targets genuinely informative samples. (c) Final accuracy: all methods converge to ∼97–98% on this 3-class dataset.\\n16'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:57610bf)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'total_pages': 19, 'format': 'PDF 1.7', 'title': 'Beyond the Loss Curve: Scaling Laws, Active Learning, and the Limits of Learning from Exact Posteriors', 'author': 'Arian Khorasani; Nathaniel Chen; Yug D Oswal; Akshat Santhana Gopalan; Egemen Kolemen; Ravid Shwartz-Ziv', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 16}, page_content='Decomposing Neural Network Error via Flow-Based Oracles\\nFigure 9. Max Epistemic consistently outperforms entropy-based selection on ImageNet-64. Selecting by exact epistemic uncertainty\\n(orange) beats both random (gray) and max entropy (blue), confirming that predictive entropy conflates aleatoric and epistemic components\\nwhile our decomposition isolates the informative signal.\\nF. Computing Posteriors at Scale\\nThe bijective invertability of normalizing flow models allow us to independently calculate samples in parallel. Posteriors are\\nobtained by iteratively cycling through smaller batches of classes per batch of image across for all possible classes. Since\\nthe results can quickly accumulate, we store the outputs of the image batch until a given size, then save the combined output\\nas a shard for each GPU. For ImageNet64, this process has been split across 7 V100 GPUs for a total of 4,200 samples. And\\nfor ImageNet128, this process has been split across 4 H200 GPUs for a total of 16,800 samples. For the Imagenet datasets,\\nwe iteratively accumulate 50 class batches per image batch which we find to be a good balance for memory efficiency.\\nOne notable difference between the models is that OracleFlow using the AFHQ 256x256 model uses a smoothing temperature\\nof 500 across 3 classes, compared to ImageNet 64x64 and ImageNet 128x128 which uses a smoothing temperature of 1 due\\nto ImageNet’s much higher class size of 1000.\\nG. Distribution Shift Experimental Details\\nWe are providing the full specification of controlled distribution-shift protocol used to stress-test supervised learners trained\\non oracle-generated data. We generate training-time shifts via two orthogonal knobs: (i) label-prior shift (class imbalance)\\nand (ii) covariate shift via additive Gaussian noise. All models are evaluated on the same held-out baseline test set to isolate\\nthe impact of training distribution mismatch.\\nG.1. Base dataset and notation\\nLet D0 = {(xi, yi)}Npool\\ni=1 denote a fixed pool of labeled images stored in the oracle raw range x ∈[−1, 1] with K = 3 classes.\\nWe also fix a baseline test set Dtest\\n0\\n(size Ntest = 2000) sampled once from the same source and used for all evaluations.\\nBaseline label distribution.\\nWe define the baseline label prior as uniform:\\nP0(y = k) = 1\\nK\\n(K = 3).\\n(5)\\nG.2. Shift family: label-prior shift and noise perturbations\\nEach shifted training distribution is parameterized by (π, σ), where π = (π0, π1, π2) specifies the target class prior and\\nσ ≥0 controls Gaussian covariate noise.\\n(A) Label-prior (class-imbalance) shift.\\nWe sample class counts via multinomial draw:\\n(n0, n1, n2) ∼Multinomial(Ntrain, π) ,\\n(6)\\nthen construct the shifted training set by sampling nk examples from the pool restricted to class k. If nk exceeds the\\navailable pool size for class k, we sample with replacement (this matches the implementation).\\n17'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:57610bf)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'total_pages': 19, 'format': 'PDF 1.7', 'title': 'Beyond the Loss Curve: Scaling Laws, Active Learning, and the Limits of Learning from Exact Posteriors', 'author': 'Arian Khorasani; Nathaniel Chen; Yug D Oswal; Akshat Santhana Gopalan; Egemen Kolemen; Ravid Shwartz-Ziv', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 17}, page_content='Decomposing Neural Network Error via Flow-Based Oracles\\nTable 6. Distribution-shift training protocol. All experiments use identical hyperparameters; only the training distribution varies.\\nComponent\\nSetting\\nBase architecture\\nResNet-50 pretrained on ImageNet-1K\\nClassifier head\\nReplace final FC with K = 3 outputs\\nTraining size\\nNtrain = 5000 (shifted)\\nValidation split\\n80/20 split of the shifted training set\\nTest set\\nBaseline test set Dtest\\n0 , Ntest = 2000\\nOptimizer\\nAdamW (weight decay 0.01)\\nLearning rate\\n10−4\\nSchedule\\nCosine annealing over 40 epochs\\nBatch size\\n32\\nSeeds\\nS = 3\\nMetric\\nTest accuracy on Dtest\\n0 ; error rate = 1 −acc\\n(B) Covariate shift via additive Gaussian noise.\\nGiven a sampled image x ∈[−1, 1], we generate a noised view:\\nx′ = clip (x + ε, −1, 1) ,\\nε ∼N(0, σ2I).\\n(7)\\nNoise is applied after selecting examples according to π and before normalization.\\nNormalization for training.\\nModels are trained using ImageNet-style normalization. Concretely, we map x′ ∈[−1, 1] to\\n[0, 1] via (x′ + 1)/2 and then apply per-channel mean/std normalization.\\nG.3. KL computation methodology\\nWe report a label-marginal KL that quantifies the strength of the prior shift:\\nKLy(Pshift(y) ∥P0(y)) =\\nK\\nX\\nk=1\\nˆπk log ˆπk\\n1/K ,\\n(8)\\nwhere ˆπk is the empirical class frequency in the constructed shifted training set (i.e., computed from the realized multinomial\\nsample and any resampling-with-replacement), and log is the natural logarithm (units: nats).\\nImportant implication.\\nKLy is insensitive to pure covariate shifts induced by σ when class priors remain balanced.\\nTherefore, we always report both (σ, KLy): KLy measures label shift strength, while σ measures the covariate-noise shift\\nstrength.\\nG.4. Per-experiment protocol (fixed across all shift settings)\\nEach shift setting (π, σ) defines one training distribution. We repeat each experiment over S = 3 random seeds (affecting\\nmultinomial draw, sampling, and optimization).\\nG.5. Shift configurations and measured KLy\\nTable 7 enumerates all perturbations. We report the target KLy implied by π (using Eq. 8 with ˆπ = π), and the empirical\\nKLy computed from realized label frequencies (mean ± std over seeds).\\nG.6. Results: performance degradation under training-time shift\\nTable 8 reports accuracy on the baseline test set Dtest\\n0\\nfor each perturbation, averaged over three seeds. We also report\\n∆TestAcc (percentage points, pp) relative to the balanced baseline, and the corresponding test error rate.\\nInterpretation (high-level).\\nAcross the tested range, label-prior shift alone (up to KLy ≈0.29 nats) causes only minor\\nchanges in baseline test accuracy (sub-0.3pp on average). In contrast, covariate noise drives substantially larger degradation,\\nwith σ = 0.10 inducing ≈2pp drops and σ = 0.15 leading to severe and high-variance failures. This separation is expected\\nbecause KLy captures label shift only, while σ controls covariate shift strength.\\n18'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:57610bf)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'total_pages': 19, 'format': 'PDF 1.7', 'title': 'Beyond the Loss Curve: Scaling Laws, Active Learning, and the Limits of Learning from Exact Posteriors', 'author': 'Arian Khorasani; Nathaniel Chen; Yug D Oswal; Akshat Santhana Gopalan; Egemen Kolemen; Ravid Shwartz-Ziv', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 18}, page_content='Decomposing Neural Network Error via Flow-Based Oracles\\nTable 7. Controlled shift configurations with exact KL values. We vary class prior π (label shift) and noise σ (covariate shift)\\nindependently. Empirical KL matches target values closely.\\nConfiguration\\nπ = (π0, π1, π2)\\nσ\\nKLy (target)\\nKLy (empirical)\\nBalanced (Baseline)\\n(0.33, 0.33, 0.34)\\n0.00\\n1.00×10−4\\n1.27×10−4 ± 1.56×10−4\\nMild Imbalance (40/35/25)\\n(0.40, 0.35, 0.25)\\n0.00\\n0.017\\n0.018 ± 0.001\\nModerate Imbalance (50/30/20)\\n(0.50, 0.30, 0.20)\\n0.00\\n0.069\\n0.070 ± 0.003\\nStrong Imbalance (60/25/15)\\n(0.60, 0.25, 0.15)\\n0.00\\n0.161\\n0.158 ± 0.007\\nVery Strong Imbalance (70/20/10)\\n(0.70, 0.20, 0.10)\\n0.00\\n0.296\\n0.293 ± 0.009\\nBalanced + Noise σ=0.05\\n(0.33, 0.33, 0.34)\\n0.05\\n1.00×10−4\\n1.27×10−4 ± 1.56×10−4\\nBalanced + Noise σ=0.10\\n(0.33, 0.33, 0.34)\\n0.10\\n1.00×10−4\\n1.27×10−4 ± 1.56×10−4\\nBalanced + Noise σ=0.15\\n(0.33, 0.33, 0.34)\\n0.15\\n1.00×10−4\\n1.27×10−4 ± 1.56×10−4\\nImbalance (50/30/20) + Noise σ=0.05\\n(0.50, 0.30, 0.20)\\n0.05\\n0.069\\n0.070 ± 0.003\\nImbalance (70/20/10) + Noise σ=0.10\\n(0.70, 0.20, 0.10)\\n0.10\\n0.296\\n0.293 ± 0.009\\nTable 8. Noise causes 20-point accuracy drops; imbalance causes <0.3 points. All models evaluated on the same baseline test set.\\nLabel-prior shift (up to KL=0.29) barely affects accuracy; covariate noise (σ=0.15) causes catastrophic degradation.\\nConfiguration\\nTest Acc (%)\\n∆TestAcc (pp)\\nTest Error (%)\\nVal Acc (%)\\nBalanced (Baseline)\\n97.67 ± 0.10\\n+0.00\\n2.33 ± 0.10\\n98.20 ± 0.26\\nMild Imbalance (40/35/25)\\n97.58 ± 0.10\\n-0.08\\n2.42 ± 0.10\\n98.10 ± 0.44\\nModerate Imbalance (50/30/20)\\n97.45 ± 0.13\\n-0.22\\n2.55 ± 0.13\\n98.40 ± 0.60\\nStrong Imbalance (60/25/15)\\n97.40 ± 0.09\\n-0.27\\n2.60 ± 0.09\\n98.70 ± 0.20\\nVery Strong Imbalance (70/20/10)\\n97.38 ± 0.20\\n-0.28\\n2.62 ± 0.20\\n98.80 ± 0.26\\nBalanced + Noise σ=0.05\\n97.58 ± 0.12\\n-0.08\\n2.42 ± 0.12\\n98.13 ± 0.35\\nBalanced + Noise σ=0.10\\n95.70 ± 0.87\\n-1.97\\n4.30 ± 0.87\\n98.00 ± 0.35\\nBalanced + Noise σ=0.15\\n76.92 ± 13.02\\n-20.75\\n23.08 ± 13.02\\n98.03 ± 0.42\\nImbalance (50/30/20) + Noise σ=0.05\\n97.03 ± 0.38\\n-0.63\\n2.97 ± 0.38\\n98.80 ± 0.44\\nImbalance (70/20/10) + Noise σ=0.10\\n95.65 ± 1.03\\n-2.02\\n4.35 ± 1.03\\n99.27 ± 0.58\\nTable 9. Isolating shift axes: prior shift is benign, covariate shift is catastrophic. Left: varying class imbalance with no noise. Right:\\nvarying noise with balanced classes. The asymmetry is stark.\\nPrior shift only (σ=0)\\nNoise shift only (balanced priors)\\nConfig\\nKLy\\nTest Acc (%)\\n∆(pp)\\nσ\\nTest Acc (%)\\n∆(pp)\\nBalanced\\n≈0\\n97.67 ± 0.10\\n+0.00\\n0.05\\n97.58 ± 0.12\\n-0.08\\n40/35/25\\n0.018\\n97.58 ± 0.10\\n-0.08\\n0.10\\n95.70 ± 0.87\\n-1.97\\n50/30/20\\n0.070\\n97.45 ± 0.13\\n-0.22\\n0.15\\n76.92 ± 13.02\\n-20.75\\n60/25/15\\n0.158\\n97.40 ± 0.09\\n-0.27\\n—\\n—\\n—\\n70/20/10\\n0.293\\n97.38 ± 0.20\\n-0.28\\n—\\n—\\n—\\n19')]"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_community.document_loaders import PyMuPDFLoader, PyPDFLoader\n",
        "\n",
        "dir_loader= DirectoryLoader(\n",
        "\n",
        "    \"../data/pdf\",\n",
        "    glob =\"**/*.pdf\",\n",
        "    loader_cls = PyMuPDFLoader,\n",
        "    show_progress=False\n",
        ")\n",
        "\n",
        "pdf_documents = dir_loader.load()\n",
        "pdf_documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "a868acbe",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "langchain_core.documents.base.Document"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(pdf_documents[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d332e96",
      "metadata": {},
      "source": [
        "Data ingestion is done \n",
        "now we will implement data embedding, chunking\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "bc8fcdac",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import chromadb\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from chromadb.config import Settings\n",
        "import uuid\n",
        "from typing import List,Dict, Any, Tuple\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "fd095a0d",
      "metadata": {},
      "outputs": [],
      "source": [
        "class EmbeddingManager:\n",
        "    def __init__(self, model_name: str= \"all-MiniLM-L6-v2\"):\n",
        "        \n",
        "\n",
        "        self.model_name = model_name\n",
        "        self.model= None\n",
        "        self._load_model()\n",
        "\n",
        "    def _load_model(self):\n",
        "        try:\n",
        "            print(f\"Loading embdedding model :{self.model_name}\")\n",
        "            self.model = SentenceTransformer(self.model_name)\n",
        "        #each text turns into n dimensions in the vector space\n",
        "            print(f\"Model loaded successfully. Embedding dimension: {self.model.get_sentence_embedding_dimension()}\")\n",
        "        except Exception as e:\n",
        "             print(f\"Error loading model: {self.model_name}: {e}\")\n",
        "             raise \n",
        "\n",
        "\n",
        "    def generate_embeddings(self,texts:List[str]) ->np.ndarray:\n",
        "\n",
        "         if not self.model:\n",
        "             raise ValueError(\"Model not loaded\")\n",
        "\n",
        "         print(f\"Generating embeddings for {len(texts) } texts...\")\n",
        "         embeddings = self.model.encode(texts,show_progress_bar = True)\n",
        "        # print(f\"Generated embeddings with shape {embeddings.shape()}\")\n",
        "         return embeddings\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "29d7746e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading embdedding model :all-MiniLM-L6-v2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading weights: 100%|██████████| 103/103 [00:00<00:00, 888.80it/s, Materializing param=pooler.dense.weight]                             \n",
            "\u001b[1mBertModel LOAD REPORT\u001b[0m from: sentence-transformers/all-MiniLM-L6-v2\n",
            "Key                     | Status     |  | \n",
            "------------------------+------------+--+-\n",
            "embeddings.position_ids | UNEXPECTED |  | \n",
            "\n",
            "\u001b[3mNotes:\n",
            "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded successfully. Embedding dimension: 384\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<__main__.EmbeddingManager at 0x18d69d9ce30>"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "embedding_manager  = EmbeddingManager()\n",
        "embedding_manager"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "6ba8fbb1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# creating the vector store\n",
        "\n",
        "class VectorStore:\n",
        "    #manages document embeddings in chromadb\n",
        "\n",
        "    def __init__(self, collection_name: str = \"pdf_documents\", persist_directory:str = \"../data/vector_store\"):\n",
        "\n",
        "        #collection_name -> name of chromadb collection\n",
        "        # directory to persist the vector store\n",
        "\n",
        "\n",
        "        self.collection_name = collection_name\n",
        "        self.persist_directory = persist_directory\n",
        "        self.client  = None\n",
        "        self.collection = None\n",
        "        self._initialize_store()\n",
        "\n",
        "    def _initialize_store(self):\n",
        "\n",
        "        # initialize chromadb client and collection\n",
        "        try:\n",
        "\n",
        "            os.makedirs(self.persist_directory,exist_ok = True)\n",
        "            self.client  = chromadb.PersistentClient(path=self.persist_directory)\n",
        "\n",
        "            # get or create collection\n",
        "\n",
        "            self.collection  = self.client.get_or_create_collection(\n",
        "\n",
        "                name = self.collection_name,\n",
        "                metadata = {\"description\":\"PDF document embeddings for RAG\"}\n",
        "            )\n",
        "\n",
        "            print(f\"Initialized vector store : {self.collection_name} successfully\")\n",
        "            print(f\"Existing documents in collection: {self.collection.count()}\")\n",
        "\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error initalizing the store : {e}\")\n",
        "            raise    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "a03287d1",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "def split_documents(documents, chunk_size = 1000,chunk_overlap=200):\n",
        "\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "\n",
        "        chunk_size = chunk_size,\n",
        "        chunk_overlap=chunk_overlap,\n",
        "        length_function = len,\n",
        "        separators =[\"\\n\\n\",\"\\n\",\" \",\"\"] \n",
        "    )\n",
        "\n",
        "\n",
        "#split the documents\n",
        "    split_docs = text_splitter.split_documents(documents)\n",
        "    print(f\"Split {len(documents)} into {len(split_docs)}\")\n",
        "\n",
        "\n",
        "# showing what a split looks like\n",
        "\n",
        "    if split_docs:\n",
        "        print(\"Example chunk produced:\\n\")\n",
        "        print(f\"Chunk content: {split_docs[0].page_content[:200]}\")\n",
        "        print(f\"Metadata: {split_docs[0].metadata}\")\n",
        "\n",
        "        return split_docs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "9b9f5929",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Split 19 into 79\n",
            "Example chunk produced:\n",
            "\n",
            "Chunk content: Beyond the Loss Curve: Scaling Laws, Active Learning, and the Limits of\n",
            "Learning from Exact Posteriors\n",
            "Arian Khorasani 1 Nathaniel Chen * 2 Yug D Oswal * 3 Akshat Santhana Gopalan 4 Egemen Kolemen 2\n",
            "R\n",
            "Metadata: {'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:57610bf)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'total_pages': 19, 'format': 'PDF 1.7', 'title': 'Beyond the Loss Curve: Scaling Laws, Active Learning, and the Limits of Learning from Exact Posteriors', 'author': 'Arian Khorasani; Nathaniel Chen; Yug D Oswal; Akshat Santhana Gopalan; Egemen Kolemen; Ravid Shwartz-Ziv', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 0}\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:57610bf)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'total_pages': 19, 'format': 'PDF 1.7', 'title': 'Beyond the Loss Curve: Scaling Laws, Active Learning, and the Limits of Learning from Exact Posteriors', 'author': 'Arian Khorasani; Nathaniel Chen; Yug D Oswal; Akshat Santhana Gopalan; Egemen Kolemen; Ravid Shwartz-Ziv', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 0}, page_content='Beyond the Loss Curve: Scaling Laws, Active Learning, and the Limits of\\nLearning from Exact Posteriors\\nArian Khorasani 1 Nathaniel Chen * 2 Yug D Oswal * 3 Akshat Santhana Gopalan 4 Egemen Kolemen 2\\nRavid Shwartz-Ziv 5\\nAbstract\\nHow close are neural networks to the best they\\ncould possibly do? Standard benchmarks can-\\nnot answer this because they lack access to the\\ntrue posterior p(y|x). We use class-conditional\\nnormalizing flows as oracles that make exact pos-\\nteriors tractable on realistic images (AFHQ, Im-\\nageNet). This enables five lines of investigation.\\nScaling laws: Prediction error decomposes into ir-\\nreducible aleatoric uncertainty and reducible epis-\\ntemic error; the epistemic component follows a\\npower law in dataset size, continuing to shrink\\neven when total loss plateaus. Limits of learn-\\ning: The aleatoric floor is exactly measurable,\\nand architectures differ markedly in how they ap-\\nproach it: ResNets exhibit clean power-law scal-'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:57610bf)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'total_pages': 19, 'format': 'PDF 1.7', 'title': 'Beyond the Loss Curve: Scaling Laws, Active Learning, and the Limits of Learning from Exact Posteriors', 'author': 'Arian Khorasani; Nathaniel Chen; Yug D Oswal; Akshat Santhana Gopalan; Egemen Kolemen; Ravid Shwartz-Ziv', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 0}, page_content='even when total loss plateaus. Limits of learn-\\ning: The aleatoric floor is exactly measurable,\\nand architectures differ markedly in how they ap-\\nproach it: ResNets exhibit clean power-law scal-\\ning while Vision Transformers stall in low-data\\nregimes. Soft labels: Oracle posteriors contain\\nlearnable structure beyond class labels: training\\nwith exact posteriors outperforms hard labels and\\nyields near-perfect calibration. Distribution shift:\\nThe oracle computes exact KL divergence of con-\\ntrolled perturbations, revealing that shift type mat-\\nters more than shift magnitude: class imbalance\\nbarely affects accuracy at divergence values where\\ninput noise causes catastrophic degradation. Ac-\\ntive learning: Exact epistemic uncertainty dis-\\ntinguishes genuinely informative samples from\\ninherently ambiguous ones, improving sample ef-\\nficiency. Our framework reveals that standard\\nmetrics hide ongoing learning, mask architectural\\ndifferences, and cannot diagnose the nature of\\ndistribution shift.'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:57610bf)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'total_pages': 19, 'format': 'PDF 1.7', 'title': 'Beyond the Loss Curve: Scaling Laws, Active Learning, and the Limits of Learning from Exact Posteriors', 'author': 'Arian Khorasani; Nathaniel Chen; Yug D Oswal; Akshat Santhana Gopalan; Egemen Kolemen; Ravid Shwartz-Ziv', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 0}, page_content='ficiency. Our framework reveals that standard\\nmetrics hide ongoing learning, mask architectural\\ndifferences, and cannot diagnose the nature of\\ndistribution shift.\\n*Equal contribution 1Mila-Quebec AI Institute, Canada 2Plasma\\nPhysics Lab, Princeton University, USA 3School of Computer Sci-\\nence and Engineering, Vellore Institute of Technology, India 4High\\nSchool Student, John P. Stevens High School, USA 5Center of Data\\nScience, New York University, New York, USA. Correspondence\\nto: Arian Khorasani <Arian.Khorasani@mila.quebec>.\\nPreprint. February 3, 2026.\\n1. Introduction\\nEvery paper reporting test accuracy implicitly asks: how\\ngood is this model? But good compared to what? Without\\naccess to the true posterior p(y|x), we cannot tell whether a\\nmodel is at 50% or 99% of the theoretical maximum. When\\na loss curve flattens, we cannot distinguish irreducible noise\\n(aleatoric uncertainty) from gaps in the model’s knowledge\\n(epistemic uncertainty) (Gal & Ghahramani, 2016; Guo'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:57610bf)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'total_pages': 19, 'format': 'PDF 1.7', 'title': 'Beyond the Loss Curve: Scaling Laws, Active Learning, and the Limits of Learning from Exact Posteriors', 'author': 'Arian Khorasani; Nathaniel Chen; Yug D Oswal; Akshat Santhana Gopalan; Egemen Kolemen; Ravid Shwartz-Ziv', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 0}, page_content='a loss curve flattens, we cannot distinguish irreducible noise\\n(aleatoric uncertainty) from gaps in the model’s knowledge\\n(epistemic uncertainty) (Gal & Ghahramani, 2016; Guo\\net al., 2017). Neural Scaling laws (Kaplan et al., 2020) tell\\nus loss decreases as N −α, but loss conflates both (Hoffmann\\net al., 2022). When performance degrades under distribu-\\ntion shift, we cannot tell whether the shift was large or\\nsmall (Hendrycks & Dietterich, 2019) (Gal & Ghahramani,\\n2016).\\nA class-conditional normalizing flow (Dinh et al., 2017;\\nKingma & Dhariwal, 2018) trained on real images provides\\na concrete reference point. The flow defines an explicit\\npθ(x | y) from which we can both sample and evaluate\\nexact likelihoods. We treat this not as an approximation\\nto nature, but as the complete specification of a synthetic\\nworld in which Bayes-optimal posteriors are computable in\\nclosed form (Dinh et al., 2017). In this world, the expected\\ncross-entropy of any classifier decomposes exactly:'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:57610bf)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'total_pages': 19, 'format': 'PDF 1.7', 'title': 'Beyond the Loss Curve: Scaling Laws, Active Learning, and the Limits of Learning from Exact Posteriors', 'author': 'Arian Khorasani; Nathaniel Chen; Yug D Oswal; Akshat Santhana Gopalan; Egemen Kolemen; Ravid Shwartz-Ziv', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 0}, page_content='world in which Bayes-optimal posteriors are computable in\\nclosed form (Dinh et al., 2017). In this world, the expected\\ncross-entropy of any classifier decomposes exactly:\\nL(qθ) = Ex[H(p(y|x))]\\n|\\n{z\\n}\\naleatoric (irreducible)\\n+ Ex[KL(p(y|x)∥qθ(y|x))]\\n|\\n{z\\n}\\nepistemic (reducible)\\nThis decomposition, intractable on any standard benchmark,\\nis a direct consequence of Bayes-optimal risk decomposition\\nand is the basis for all our experiments. As illustrated in\\nFigure 1, we train state-of-the-art normalizing flows on\\nrealistic image datasets and conduct a thorough validation\\nto confirm the generated images match real data statistics\\nand are not memorized (Section 2.3). We find:\\n• Scaling laws beyond the loss curve. Epistemic error\\nfollows a power law KL ∝N −α that continues to\\nshrink even when total loss plateaus. The exponents\\ndiffer across architectures.\\n• The limits of learning are exactly measurable. The\\naleatoric floor is a hard limit no architecture can beat.\\n1'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:57610bf)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'total_pages': 19, 'format': 'PDF 1.7', 'title': 'Beyond the Loss Curve: Scaling Laws, Active Learning, and the Limits of Learning from Exact Posteriors', 'author': 'Arian Khorasani; Nathaniel Chen; Yug D Oswal; Akshat Santhana Gopalan; Egemen Kolemen; Ravid Shwartz-Ziv', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 0}, page_content='shrink even when total loss plateaus. The exponents\\ndiffer across architectures.\\n• The limits of learning are exactly measurable. The\\naleatoric floor is a hard limit no architecture can beat.\\n1\\narXiv:2602.00315v1  [cs.LG]  30 Jan 2026'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:57610bf)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'total_pages': 19, 'format': 'PDF 1.7', 'title': 'Beyond the Loss Curve: Scaling Laws, Active Learning, and the Limits of Learning from Exact Posteriors', 'author': 'Arian Khorasani; Nathaniel Chen; Yug D Oswal; Akshat Santhana Gopalan; Egemen Kolemen; Ravid Shwartz-Ziv', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 1}, page_content='Decomposing Neural Network Error via Flow-Based Oracles\\nFigure 1. Normalizing flows enable exact posterior computation on realistic images. (Left) We train class-conditional flows on\\nAFHQ/ImageNet, creating a frozen oracle with tractable likelihoods. (Center) The oracle decomposes prediction error into aleatoric\\n(irreducible) and epistemic (reducible) components, quantities hidden in standard benchmarks. (Right) We apply this framework to\\ndiagnose scaling laws, quantify distribution shift with exact KL divergence, and train models with exact soft labels.\\nArchitectures differ markedly in how they approach it:\\nResNets show clean power-law scaling while Vision\\nTransformers stall without pretraining.\\n• Soft labels from exact posteriors enable better learn-\\ning. Training with p(y|x) instead of argmax labels\\noutperforms hard labels and yields near-perfect calibra-\\ntion (Szegedy et al., 2015).\\n• Distribution shift is exactly quantifiable. The oracle'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:57610bf)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'total_pages': 19, 'format': 'PDF 1.7', 'title': 'Beyond the Loss Curve: Scaling Laws, Active Learning, and the Limits of Learning from Exact Posteriors', 'author': 'Arian Khorasani; Nathaniel Chen; Yug D Oswal; Akshat Santhana Gopalan; Egemen Kolemen; Ravid Shwartz-Ziv', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 1}, page_content='ing. Training with p(y|x) instead of argmax labels\\noutperforms hard labels and yields near-perfect calibra-\\ntion (Szegedy et al., 2015).\\n• Distribution shift is exactly quantifiable. The oracle\\ncomputes KL[pshifted∥pbaseline] exactly, revealing that\\nshift type matters far more than magnitude: class im-\\nbalance barely affects accuracy at divergence values\\nwhere input noise causes collapse.\\n• Active learning with ground-truth uncertainty. Ex-\\nact epistemic uncertainty distinguishes genuinely in-\\nformative samples from inherently ambiguous ones,\\nimproving sample efficiency over standard acquisition\\nfunctions that confound aleatoric and epistemic uncer-\\ntainty (Gal et al., 2017).\\nOur oracle defines a synthetic world, not nature. But the phe-\\nnomena it reveals (hidden epistemic progress, architectural\\nscaling gaps, misleading divergence measures) are proper-\\nties of the models and metrics, not the data source (Belghazi\\net al., 2021).\\n2. The Oracle Framework'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:57610bf)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'total_pages': 19, 'format': 'PDF 1.7', 'title': 'Beyond the Loss Curve: Scaling Laws, Active Learning, and the Limits of Learning from Exact Posteriors', 'author': 'Arian Khorasani; Nathaniel Chen; Yug D Oswal; Akshat Santhana Gopalan; Egemen Kolemen; Ravid Shwartz-Ziv', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 1}, page_content='scaling gaps, misleading divergence measures) are proper-\\nties of the models and metrics, not the data source (Belghazi\\net al., 2021).\\n2. The Oracle Framework\\nWe build an oracle benchmark by training class-conditional\\nnormalizing flows that define tractable densities pθ(x | y) on\\nhigh-dimensional inputs (Papamakarios et al., 2021). Once\\nthe flows are trained and frozen, they specify a synthetic\\nworld in which the posterior is available by Bayes’ rule,\\np(y | x) =\\npθ(x | y) πy\\nP\\ny′ pθ(x | y′) πy′ .\\n(1)\\nBecause likelihoods are tractable, we can compute Bayes-\\noptimal posteriors exactly under the oracle model (up to\\nfloating-point arithmetic) via Bayes’ rule (Dinh et al., 2017).\\nThis in turn makes our loss decomposition and controlled\\nscaling, shift, and active-learning experiments possible.\\nPractical details on training and scaling to many classes\\n(ImageNet) are provided in the appendix (Deng et al., 2009).\\n2.1. Flows as Oracles\\nA normalizing flow defines an invertible mapping between'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:57610bf)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'total_pages': 19, 'format': 'PDF 1.7', 'title': 'Beyond the Loss Curve: Scaling Laws, Active Learning, and the Limits of Learning from Exact Posteriors', 'author': 'Arian Khorasani; Nathaniel Chen; Yug D Oswal; Akshat Santhana Gopalan; Egemen Kolemen; Ravid Shwartz-Ziv', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 1}, page_content='Practical details on training and scaling to many classes\\n(ImageNet) are provided in the appendix (Deng et al., 2009).\\n2.1. Flows as Oracles\\nA normalizing flow defines an invertible mapping between\\ninputs x and latent variables z (Dinh et al., 2017; Kingma &\\nDhariwal, 2018): the forward map fθ sends x 7→z, while\\nthe inverse f −1\\nθ\\ngenerates samples by mapping z 7→x for\\nz ∼N(0, I). The key property is that densities are tractable\\n(Papamakarios et al., 2021):\\nlog pθ(x) = log pZ(fθ(x)) + log |det Jfθ(x)| .\\n(2)\\n2'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:57610bf)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'total_pages': 19, 'format': 'PDF 1.7', 'title': 'Beyond the Loss Curve: Scaling Laws, Active Learning, and the Limits of Learning from Exact Posteriors', 'author': 'Arian Khorasani; Nathaniel Chen; Yug D Oswal; Akshat Santhana Gopalan; Egemen Kolemen; Ravid Shwartz-Ziv', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 2}, page_content='Decomposing Neural Network Error via Flow-Based Oracles\\nWe train one flow per class, yielding class-conditional densi-\\nties pθ(x | y), and then freeze the parameters. For any input\\nx, we compute the oracle posterior via Bayes’ rule (Eq. 1).\\nUnlike typical Bayesian methods that rely on sampling or\\nvariational approximations, this computation is exact under\\nthe oracle model (Alemi et al., 2019) since the likelihoods\\nand class prior are explicitly known: given the trained flows\\nand the chosen class prior π, p(y | x) is computed in closed\\nform (implemented in log space with log-sum-exp normal-\\nization for numerical stability). The oracle therefore returns\\nthe true posterior for the synthetic world defined by the flow.\\n2.2. What Does “Truth” Mean Here?\\nHere, “true” means Bayes-optimal with respect to the oracle\\nworld induced by the trained flows. The oracle world is\\ndefined by (i) the per-class flow densities pθ(x | y), (ii) the\\nexact preprocessing pipeline used during flow training and'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:57610bf)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'total_pages': 19, 'format': 'PDF 1.7', 'title': 'Beyond the Loss Curve: Scaling Laws, Active Learning, and the Limits of Learning from Exact Posteriors', 'author': 'Arian Khorasani; Nathaniel Chen; Yug D Oswal; Akshat Santhana Gopalan; Egemen Kolemen; Ravid Shwartz-Ziv', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 2}, page_content='world induced by the trained flows. The oracle world is\\ndefined by (i) the per-class flow densities pθ(x | y), (ii) the\\nexact preprocessing pipeline used during flow training and\\nsampling, and (iii) a class prior πy. All “exact” statements\\nare with respect to this specification: for the oracle world,\\nwe can compute the Bayes posterior p(y | x) (Eq. 1) and\\ntherefore quantities such as the aleatoric term E[H(p(y |\\nx))] and epistemic term E[KL(p ∥qθ)] exactly under the\\noracle model (Section 3).\\nUnless otherwise stated, we use a uniform prior over classes\\n(πy = 1/K). In the class-imbalance shift experiments, we\\nmodify the class prior π while keeping pθ(x | y) fixed\\n(Chawla et al., 2002), isolating prior shift from conditional\\nshift.\\nThe main limitation is external validity: conclusions are\\nonly useful insofar as samples from the oracle world match\\nthe statistical and semantic properties of the real data used\\nto fit the flows.\\n2.3. Oracle Validation'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:57610bf)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'total_pages': 19, 'format': 'PDF 1.7', 'title': 'Beyond the Loss Curve: Scaling Laws, Active Learning, and the Limits of Learning from Exact Posteriors', 'author': 'Arian Khorasani; Nathaniel Chen; Yug D Oswal; Akshat Santhana Gopalan; Egemen Kolemen; Ravid Shwartz-Ziv', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 2}, page_content='only useful insofar as samples from the oracle world match\\nthe statistical and semantic properties of the real data used\\nto fit the flows.\\n2.3. Oracle Validation\\nBottom line: Generated images are statistically similar\\nto the original data across six metrics, and classifiers\\ntrained on oracle samples approach the Bayes-optimal\\nbound (Salimans et al., 2016).\\nBecause the oracle provides exact posteriors, we can com-\\npute the Bayes-optimal accuracy for the oracle world:\\n99.77% on AFHQ (Choi et al., 2020) (Bayes error 0.23%).\\nIf classifiers trained on oracle data approach this bound,\\nthe oracle is producing learnable, well-structured sam-\\nples. We trained five architectures on 50K oracle samples.\\nConvNeXt (Liu et al., 2022) reaches 98.0%, ResNet (He\\net al., 2015) 97.7%, Swin (Liu et al., 2021) 97.7%, Mo-\\nbileNet (Howard et al., 2019) 97.7%, and ViT-Base (Doso-\\nvitskiy et al., 2021) 97.0% (Table 1).\\nThe remaining\\n1.8–2.8% gap closes with longer training. On ImageNet-'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:57610bf)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'total_pages': 19, 'format': 'PDF 1.7', 'title': 'Beyond the Loss Curve: Scaling Laws, Active Learning, and the Limits of Learning from Exact Posteriors', 'author': 'Arian Khorasani; Nathaniel Chen; Yug D Oswal; Akshat Santhana Gopalan; Egemen Kolemen; Ravid Shwartz-Ziv', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 2}, page_content='bileNet (Howard et al., 2019) 97.7%, and ViT-Base (Doso-\\nvitskiy et al., 2021) 97.0% (Table 1).\\nThe remaining\\n1.8–2.8% gap closes with longer training. On ImageNet-\\n64 (Chrabaszcz et al., 2017), the Bayes error is 4.031%.\\nTable 1. Classifiers approach the Bayes-optimal bound on\\noracle data. Self-validation on AFHQ: all architectures reach 97–\\n98% accuracy, within 2–3% of the theoretical optimum (99.8%).\\nArchitecture\\nAccuracy (%)\\nError (%)\\nGap (%)\\nConvNeXt\\n98.03\\n1.97\\n1.75\\nResNet-18\\n97.73\\n2.27\\n2.05\\nSwin\\n97.68\\n2.32\\n2.10\\nMobileNet\\n97.65\\n2.35\\n2.12\\nViT-Base\\n96.98\\n3.02\\n2.80\\nWe validated distributional quality using six metrics.\\nFID (Heusel et al., 2018) measures feature distribution dis-\\ntance (28.44 on AFHQ, 13.48 on ImageNet-64), higher\\nthan diffusion models because flows trade sharpness for ex-\\nact likelihoods. Inception Score (Salimans et al., 2016)\\nevaluates quality and diversity (6.24 ± 3.57 on AFHQ,\\n32.57 ± 4.47 on ImageNet-64). Manifold coverage con-'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:57610bf)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'total_pages': 19, 'format': 'PDF 1.7', 'title': 'Beyond the Loss Curve: Scaling Laws, Active Learning, and the Limits of Learning from Exact Posteriors', 'author': 'Arian Khorasani; Nathaniel Chen; Yug D Oswal; Akshat Santhana Gopalan; Egemen Kolemen; Ravid Shwartz-Ziv', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 2}, page_content='act likelihoods. Inception Score (Salimans et al., 2016)\\nevaluates quality and diversity (6.24 ± 3.57 on AFHQ,\\n32.57 ± 4.47 on ImageNet-64). Manifold coverage con-\\nfirms broad support (90% on AFHQ, 89.9% on ImageNet-\\n64). Feature variance match shows semantic diversity\\nis preserved (83–92% on AFHQ, 72–93% on ImageNet-\\n64). Memorization check: only 36% of AFHQ samples\\n(6.5% on ImageNet-64) have a training neighbor within fea-\\nture distance 10, and visual inspection confirms these share\\npose/lighting but depict distinct individuals. Full metrics\\nand protocol in Appendix A, Table 4.\\n2.4. Implementation\\nWe use TarFlow (Zhai et al., 2025), a recent normalizing\\nflow that achieves strong likelihood scores on AFHQ and Im-\\nageNet. Training follows standard practice: dequantization,\\nlogit preprocessing, and maximum likelihood optimization.\\nWe train separate flows per class on AFHQ (3 classes: cat,\\ndog, wild) and ImageNet (Deng et al., 2009) (up to 1000'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:57610bf)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'total_pages': 19, 'format': 'PDF 1.7', 'title': 'Beyond the Loss Curve: Scaling Laws, Active Learning, and the Limits of Learning from Exact Posteriors', 'author': 'Arian Khorasani; Nathaniel Chen; Yug D Oswal; Akshat Santhana Gopalan; Egemen Kolemen; Ravid Shwartz-Ziv', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 2}, page_content='logit preprocessing, and maximum likelihood optimization.\\nWe train separate flows per class on AFHQ (3 classes: cat,\\ndog, wild) and ImageNet (Deng et al., 2009) (up to 1000\\nclasses at 64×64 and 128×128 resolution). After training,\\nwe freeze the flows and generate 50,000 labeled samples\\nwith oracle posteriors. We compute posteriors in log space\\nand normalize with log-sum-exp for numerical stability. Full\\narchitecture and systems details (including throughput and\\nscaling across many classes) appear in the appendix.\\n3. Decomposing Prediction Error\\nWhy does this decomposition matter? In practice, a flatten-\\ning loss curve is ambiguous: it could mean the model has\\nlearned everything learnable, or it could mean the model is\\nstuck while reducible error remains. Distinguishing these\\ncases determines whether more data, more capacity, or a\\ndifferent task formulation is needed. Similarly, calibration\\nmethods aim to match model confidence to true probabil-'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:57610bf)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'total_pages': 19, 'format': 'PDF 1.7', 'title': 'Beyond the Loss Curve: Scaling Laws, Active Learning, and the Limits of Learning from Exact Posteriors', 'author': 'Arian Khorasani; Nathaniel Chen; Yug D Oswal; Akshat Santhana Gopalan; Egemen Kolemen; Ravid Shwartz-Ziv', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 2}, page_content='cases determines whether more data, more capacity, or a\\ndifferent task formulation is needed. Similarly, calibration\\nmethods aim to match model confidence to true probabil-\\nities, but without access to true posteriors, calibration can\\nonly be evaluated against empirical frequencies (Guo et al.,\\n3'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:57610bf)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'total_pages': 19, 'format': 'PDF 1.7', 'title': 'Beyond the Loss Curve: Scaling Laws, Active Learning, and the Limits of Learning from Exact Posteriors', 'author': 'Arian Khorasani; Nathaniel Chen; Yug D Oswal; Akshat Santhana Gopalan; Egemen Kolemen; Ravid Shwartz-Ziv', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 3}, page_content='Decomposing Neural Network Error via Flow-Based Oracles\\n2017). Access to the true posterior p(y|x) enables a decom-\\nposition that resolves both questions. Consider a classifier\\nqθ(y|x) and measure its expected cross-entropy:\\nL(qθ) = Ex\\n\"\\n−\\nX\\ny\\np(y|x) log qθ(y|x)\\n#\\n(3)\\nThis loss splits cleanly into two terms:\\nL(qθ) = Ex [H(p(y|x))]\\n|\\n{z\\n}\\naleatoric\\n+ Ex [KL(p(y|x)∥qθ(y|x))]\\n|\\n{z\\n}\\nepistemic\\n(4)\\nThe aleatoric term is the entropy of the true posterior aver-\\naged over inputs. It reflects irreducible uncertainty, or the\\nambiguity inherent in the data that no classifier can resolve.\\nAn image of a cat-like dog has high H(p(y|x)) regardless\\nof model quality.\\nThe epistemic term measures the gap between the model’s\\nbeliefs and truth (Gal & Ghahramani, 2016). A perfect\\nmodel achieves KL = 0; any deviation indicates something\\nlearnable that the model has not yet captured.\\nOn standard benchmarks, we observe only total loss. When\\nit plateaus, we cannot tell whether the model has reached the'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:57610bf)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'total_pages': 19, 'format': 'PDF 1.7', 'title': 'Beyond the Loss Curve: Scaling Laws, Active Learning, and the Limits of Learning from Exact Posteriors', 'author': 'Arian Khorasani; Nathaniel Chen; Yug D Oswal; Akshat Santhana Gopalan; Egemen Kolemen; Ravid Shwartz-Ziv', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 3}, page_content='learnable that the model has not yet captured.\\nOn standard benchmarks, we observe only total loss. When\\nit plateaus, we cannot tell whether the model has reached the\\naleatoric floor or whether epistemic error remains. Our ora-\\ncle makes both terms measurable. This distinction matters:\\nif epistemic error dominates, more data or capacity should\\nhelp; if aleatoric error dominates, improvements require\\nchanging the task itself.\\n4. Experiments\\nSetup.\\nWe trained TarFlow (Zhai et al., 2025) on\\nAFHQ (Choi et al., 2020) (3 classes: cat, dog, wild; ∼15K\\nimages) and ImageNet (Deng et al., 2009) at 64×64 res-\\nolution. Oracle quality is validated in Section 2.3. We\\ngenerated labeled samples with exact posteriors for all exper-\\niments. We trained classifiers (ResNet-50 (He et al., 2015),\\nViT-Base (Dosovitskiy et al., 2021), ConvNeXt (Liu et al.,\\n2022), Swin (Liu et al., 2021), MobileNetV3 (Howard et al.,\\n2019)) using default hyperparameters on varying amounts of'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:57610bf)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'total_pages': 19, 'format': 'PDF 1.7', 'title': 'Beyond the Loss Curve: Scaling Laws, Active Learning, and the Limits of Learning from Exact Posteriors', 'author': 'Arian Khorasani; Nathaniel Chen; Yug D Oswal; Akshat Santhana Gopalan; Egemen Kolemen; Ravid Shwartz-Ziv', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 3}, page_content='ViT-Base (Dosovitskiy et al., 2021), ConvNeXt (Liu et al.,\\n2022), Swin (Liu et al., 2021), MobileNetV3 (Howard et al.,\\n2019)) using default hyperparameters on varying amounts of\\noracle data, from N=100 to N=10,000 (up to N=45, 000\\nfor ImageNet-64) samples. All results are averaged over 3\\nseeds with standard deviation shown.\\n4.1. Scaling Laws Beyond the Loss Curve\\nBottom line: Epistemic error follows a clean power law\\nin dataset size, continuing to shrink even when total loss\\nplateaus. Standard metrics hide this ongoing learning.\\nWe trained classifiers on varying amounts of oracle\\ndata and decomposed prediction error into aleatoric\\nand epistemic components (Kaplan et al., 2020) (Sec-\\ntion 3). For each model, we computed total cross-entropy\\nloss (what standard benchmarks measure), the aleatoric\\ncomponent E[H(p(y|x))], and the epistemic component\\nE[KL(p(y|x)∥qθ(y|x))].\\nFigure 2 shows the result. Total cross-entropy loss decreases'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:57610bf)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'total_pages': 19, 'format': 'PDF 1.7', 'title': 'Beyond the Loss Curve: Scaling Laws, Active Learning, and the Limits of Learning from Exact Posteriors', 'author': 'Arian Khorasani; Nathaniel Chen; Yug D Oswal; Akshat Santhana Gopalan; Egemen Kolemen; Ravid Shwartz-Ziv', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 3}, page_content='loss (what standard benchmarks measure), the aleatoric\\ncomponent E[H(p(y|x))], and the epistemic component\\nE[KL(p(y|x)∥qθ(y|x))].\\nFigure 2 shows the result. Total cross-entropy loss decreases\\nwith dataset size on a log-log scale, consistent with Kaplan\\net al. (2020). MobileNet exhibits the highest loss throughout,\\nResNet and ConvNeXt the lowest. But the total loss curve\\nobscures what is actually happening.\\nThe epistemic component behaves differently. On a log-log\\nplot, it traces a nearly perfect straight line: KL ∝N −α\\n(dashed lines in Figure 2b) (Hoffmann et al., 2022). This\\ndecay continues even when total loss appears to plateau,\\nbecause the aleatoric floor dominates the total at large N.\\nStandard metrics miss this ongoing learning because they\\nconflate the two error sources.\\nThe scaling exponents α differ across architectures (Fig-\\nure 2d): ResNet-50 (α=0.039 ± 0.007), ViT-Base (0.032 ±\\n0.010), ConvNeXt (0.030 ± 0.008), Swin (0.030 ± 0.005),'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:57610bf)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'total_pages': 19, 'format': 'PDF 1.7', 'title': 'Beyond the Loss Curve: Scaling Laws, Active Learning, and the Limits of Learning from Exact Posteriors', 'author': 'Arian Khorasani; Nathaniel Chen; Yug D Oswal; Akshat Santhana Gopalan; Egemen Kolemen; Ravid Shwartz-Ziv', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 3}, page_content='The scaling exponents α differ across architectures (Fig-\\nure 2d): ResNet-50 (α=0.039 ± 0.007), ViT-Base (0.032 ±\\n0.010), ConvNeXt (0.030 ± 0.008), Swin (0.030 ± 0.005),\\nand MobileNetV3 (0.135 ± 0.025). These quantify how\\nefficiently each architecture converts additional data into\\nreduced uncertainty; MobileNet’s much higher α indicates\\nfaster epistemic decay rate, yet it starts from a higher error\\nfloor, suggesting a less efficient initial representation. Cali-\\nbration error (ECE) also decreases with N, but at a different\\nrate than epistemic error, confirming that calibration and pos-\\nterior approximation quality are distinct phenomena (Guo\\net al., 2017).\\nThe aleatoric floor.\\nOn AFHQ, the aleatoric uncertainty\\nis exactly 0.012 nats (Bayes error 0.23%; see Section 2.3),\\nwith mutual information I(X; Y ) = 1.577 bits (normalized\\nMI = 99.5%). On ImageNet-64, the aleatoric floor is 0.10\\nnats (Bayes error 4.031%), higher due to the greater number'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:57610bf)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'total_pages': 19, 'format': 'PDF 1.7', 'title': 'Beyond the Loss Curve: Scaling Laws, Active Learning, and the Limits of Learning from Exact Posteriors', 'author': 'Arian Khorasani; Nathaniel Chen; Yug D Oswal; Akshat Santhana Gopalan; Egemen Kolemen; Ravid Shwartz-Ziv', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 3}, page_content='with mutual information I(X; Y ) = 1.577 bits (normalized\\nMI = 99.5%). On ImageNet-64, the aleatoric floor is 0.10\\nnats (Bayes error 4.031%), higher due to the greater number\\nof classes and inherent inter-class ambiguity.\\nImageNet replication.\\nFigure 6 shows scaling laws on\\nImageNet-64 up to 45K samples, a substantially more chal-\\nlenging dataset than AFHQ due to its fine-grained structure\\nand 1000-way label space. Despite this higher intrinsic en-\\ntropy, the total cross-entropy loss decreases rapidly with\\ndataset size, at a faster absolute rate than in AFHQ, rein-\\nforcing the classical finding that larger datasets continue to\\nyield measurable gains even when the absolute loss remains\\ndominated by intrinsic class complexity.\\nThe aleatoric component remains effectively constant at\\napproximately 0.10 nats across all dataset sizes, confirming\\nthat all models converge to the same irreducible uncertainty\\ninduced by the oracle conditional distribution. As in AFHQ,'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:57610bf)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'total_pages': 19, 'format': 'PDF 1.7', 'title': 'Beyond the Loss Curve: Scaling Laws, Active Learning, and the Limits of Learning from Exact Posteriors', 'author': 'Arian Khorasani; Nathaniel Chen; Yug D Oswal; Akshat Santhana Gopalan; Egemen Kolemen; Ravid Shwartz-Ziv', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 3}, page_content='approximately 0.10 nats across all dataset sizes, confirming\\nthat all models converge to the same irreducible uncertainty\\ninduced by the oracle conditional distribution. As in AFHQ,\\nthis floor is architecture-independent and invariant to dataset\\nsize, reflecting the Bayes error of the oracle world rather\\nthan model capacity. The higher value relative to AFHQ\\n(0.012 nats) is consistent with ImageNet-64’s greater inter-\\n4'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:57610bf)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'total_pages': 19, 'format': 'PDF 1.7', 'title': 'Beyond the Loss Curve: Scaling Laws, Active Learning, and the Limits of Learning from Exact Posteriors', 'author': 'Arian Khorasani; Nathaniel Chen; Yug D Oswal; Akshat Santhana Gopalan; Egemen Kolemen; Ravid Shwartz-Ziv', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 4}, page_content='Decomposing Neural Network Error via Flow-Based Oracles\\nFigure 2. Epistemic error follows a power law even when total loss plateaus. (a) Total cross-entropy decreases with dataset size;\\nMobileNet shows the highest loss, ResNet/ConvNeXt the lowest. (b) Epistemic uncertainty (KL from oracle) follows N −α; dashed\\nlines are power-law fits. This decay continues even when total loss appears flat. (c) Accuracy improves from 92–96% at N=100 toward\\n97–98% at N=10,000. (d) Scaling exponents reveal architectural differences: MobileNet improves fastest (α=0.135), ViT stalls without\\npretraining (α=0.032).\\nclass overlap and multimodality.\\nEpistemic uncertainty decays with dataset size and cor-\\nroborates the power-law behaviour observed on AFHQ.\\nImageNet-64, however, exhibits a brief plateau in the mid-\\nrange (2,000-10,000 samples), reflecting a representation-\\ntransition regime in which models do not immediately con-\\nvert additional data into reduced epistemic uncertainty. Be-'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:57610bf)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'total_pages': 19, 'format': 'PDF 1.7', 'title': 'Beyond the Loss Curve: Scaling Laws, Active Learning, and the Limits of Learning from Exact Posteriors', 'author': 'Arian Khorasani; Nathaniel Chen; Yug D Oswal; Akshat Santhana Gopalan; Egemen Kolemen; Ravid Shwartz-Ziv', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 4}, page_content='range (2,000-10,000 samples), reflecting a representation-\\ntransition regime in which models do not immediately con-\\nvert additional data into reduced epistemic uncertainty. Be-\\nyond this regime, all architectures resume clear power-law\\ndecay, reinforcing that epistemic uncertainty, alongside total\\nloss, is a distinct and reliable indicator of continued learning\\nat scale.\\nThe resulting exponents show both similarities and diver-\\ngences relative to AFHQ. ResNet-50 exhibits the highest\\nscaling rate (α=0.019 ± 0.006), closely followed by ViT-\\nBase (0.018 ± 0.004), mirroring their behaviour on AFHQ.\\nConvNeXt achieves only a weak decay rate (0.003 ± 0.003,\\nlower than ResNet and ViT-Base as in AFHQ), indicating\\nlimited epistemic improvement per additional sample, likely\\ndue to architectural biases that mismatch the ImageNet-64\\nfeature geometry. Swin-T displays the strongest positive\\nexponent (0.027 ± 0.01), consistent with hierarchical trans-'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:57610bf)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'total_pages': 19, 'format': 'PDF 1.7', 'title': 'Beyond the Loss Curve: Scaling Laws, Active Learning, and the Limits of Learning from Exact Posteriors', 'author': 'Arian Khorasani; Nathaniel Chen; Yug D Oswal; Akshat Santhana Gopalan; Egemen Kolemen; Ravid Shwartz-Ziv', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 4}, page_content='due to architectural biases that mismatch the ImageNet-64\\nfeature geometry. Swin-T displays the strongest positive\\nexponent (0.027 ± 0.01), consistent with hierarchical trans-\\nformers benefiting disproportionately from larger, more di-\\nverse datasets.\\nMobileNet-V3 is an exception: its epistemic curve exhibits\\na pronounced non-monotonicity. We attribute this to Mo-\\nbileNet’s compressed architecture and depthwise-separable\\nconvolutions, which yield an inefficient initial representa-\\ntion incapable of faithfully capturing ImageNet-64’s high\\ninter-class diversity. At small dataset sizes, the model over-\\nfits and becomes overconfident, producing an artificially\\nlow epistemic floor. As the dataset expands into the mid-\\nscale regime (≈2k–10k samples), this brittle representation\\nis forced to reorganize, triggering a spike in loss, calibra-\\ntion error, and epistemic uncertainty. Once past this regime,\\nMobileNet resumes clean power-law decay, confirming that'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:57610bf)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'total_pages': 19, 'format': 'PDF 1.7', 'title': 'Beyond the Loss Curve: Scaling Laws, Active Learning, and the Limits of Learning from Exact Posteriors', 'author': 'Arian Khorasani; Nathaniel Chen; Yug D Oswal; Akshat Santhana Gopalan; Egemen Kolemen; Ravid Shwartz-Ziv', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 4}, page_content='is forced to reorganize, triggering a spike in loss, calibra-\\ntion error, and epistemic uncertainty. Once past this regime,\\nMobileNet resumes clean power-law decay, confirming that\\nthe underlying epistemic scaling law still holds once an\\nadequate representation is established.\\nTogether with the monotonic rise in test accuracy, these\\ntrends show that accuracy, calibration, and epistemic uncer-\\ntainty respond differently to data scaling, with calibration\\nshowing partial signatures of the behaviour made explicit\\nby epistemic uncertainty.\\nArchitectures differ in how they approach optimality.\\nTable 2 shows that ResNet-18 exhibits clean power-law\\nscaling: epistemic error drops from 0.16 to 0.026 as data\\nincreases from N=100 to N=40,000 (a 6× reduction). ViT-\\nBase behaves differently: it starts competitive at N=100 but\\n5'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:57610bf)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'total_pages': 19, 'format': 'PDF 1.7', 'title': 'Beyond the Loss Curve: Scaling Laws, Active Learning, and the Limits of Learning from Exact Posteriors', 'author': 'Arian Khorasani; Nathaniel Chen; Yug D Oswal; Akshat Santhana Gopalan; Egemen Kolemen; Ravid Shwartz-Ziv', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 5}, page_content='Decomposing Neural Network Error via Flow-Based Oracles\\nTable 2. ResNets reduce epistemic error 6×; ViTs stall with-\\nout pretraining. Epistemic KL (nats) vs. dataset size on AFHQ.\\nResNet exhibits clean power-law scaling from 0.16 to 0.026; ViT\\nimproves by only 1.2× over the same range.\\nDataset size N\\nResNet-18\\nViT-B-16\\n100\\n0.160\\n0.131\\n1,000\\n0.090\\n0.141\\n5,000\\n0.058\\n0.099\\n10,000\\n0.033\\n0.117\\n40,000\\n0.026\\n0.117\\nthen stalls. By N=40,000, ViT has improved by less than\\n1.2×. Both architectures achieve similar total loss at large\\nN; the difference is in how they approach optimality. ViTs,\\nlacking pretraining, fail to extract geometric structure from\\nsmall samples (Dosovitskiy et al., 2021). This is consistent\\nwith Dosovitskiy et al. (2021), but our framework quantifies\\nthe gap in information-theoretic terms.\\n4.2. Soft Labels Enable Better Learning\\nOur oracle can generate exact posterior distributions p(y|x)\\nas training labels, not just the argmax class. Training with'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:57610bf)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'total_pages': 19, 'format': 'PDF 1.7', 'title': 'Beyond the Loss Curve: Scaling Laws, Active Learning, and the Limits of Learning from Exact Posteriors', 'author': 'Arian Khorasani; Nathaniel Chen; Yug D Oswal; Akshat Santhana Gopalan; Egemen Kolemen; Ravid Shwartz-Ziv', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 5}, page_content='4.2. Soft Labels Enable Better Learning\\nOur oracle can generate exact posterior distributions p(y|x)\\nas training labels, not just the argmax class. Training with\\nthese soft labels outperforms hard-label training at 4 out of\\n5 dataset sizes, with accuracy gains up to ∼1%. Models\\ntrained on soft labels also achieve near-perfect calibration\\n(ECE = 0.018) (Guo et al., 2017), confirming the posteriors\\nencode learnable structure beyond class labels (Szegedy\\net al., 2015; Hinton et al., 2015). Full results in Appendix D.\\n4.3. Distribution Shift: Exact Quantification via Oracle\\nBottom line: The oracle computes exact KL divergence\\nof controlled perturbations. Class imbalance barely af-\\nfects accuracy at KL values where input noise causes\\ncollapse. Shift type matters far more than shift magni-\\ntude.\\nOn standard benchmarks, distribution shift is observed but\\nnever measured: we see accuracy drop but cannot quantify\\nhow much the distribution actually changed. Our oracle'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:57610bf)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'total_pages': 19, 'format': 'PDF 1.7', 'title': 'Beyond the Loss Curve: Scaling Laws, Active Learning, and the Limits of Learning from Exact Posteriors', 'author': 'Arian Khorasani; Nathaniel Chen; Yug D Oswal; Akshat Santhana Gopalan; Egemen Kolemen; Ravid Shwartz-Ziv', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 5}, page_content='tude.\\nOn standard benchmarks, distribution shift is observed but\\nnever measured: we see accuracy drop but cannot quantify\\nhow much the distribution actually changed. Our oracle\\ncomputes KL[pshifted∥pbaseline] exactly by evaluating the true\\nlog-likelihood log p(x|y) under both distributions. This\\nenables controlled experiments that are impossible on real\\nbenchmarks: we introduce perturbations of known type and\\nmagnitude, measure the exact divergence, and observe how\\nmodels degrade.\\nWe introduce two types of controlled perturbation to the\\noracle distribution:\\n• Class imbalance: Shifting the class prior from uni-\\nform to skewed ratios (e.g., 40/35/25 up to 70/20/10),\\nwhich increases KL divergence through the marginal\\np(y) (Chawla et al., 2002).\\n• Gaussian noise: Adding noise with standard deviation\\nσ ∈{0.05, 0.10, 0.15} to generated images, which in-\\ncreases KL divergence through the conditional p(x|y).\\nThe results (Figure 3) show a clear asymmetry. Class im-'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:57610bf)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'total_pages': 19, 'format': 'PDF 1.7', 'title': 'Beyond the Loss Curve: Scaling Laws, Active Learning, and the Limits of Learning from Exact Posteriors', 'author': 'Arian Khorasani; Nathaniel Chen; Yug D Oswal; Akshat Santhana Gopalan; Egemen Kolemen; Ravid Shwartz-Ziv', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 5}, page_content='σ ∈{0.05, 0.10, 0.15} to generated images, which in-\\ncreases KL divergence through the conditional p(x|y).\\nThe results (Figure 3) show a clear asymmetry. Class im-\\nbalance produces KL divergences from 0.018 to 0.293, yet\\naccuracy remains flat at 97.4–97.7%. Gaussian noise at\\nσ=0.15 produces a comparable KL divergence but causes\\naccuracy to collapse to ∼77%, a 20-percentage-point drop.\\nThe same KL magnitude leads to dramatically different\\noutcomes depending on where the shift occurs.\\nClass imbalance shifts the marginal p(y) but leaves the\\nclass-conditional p(x|y) intact, so learned features remain\\ndiscriminative. Gaussian noise corrupts p(x|y) directly, de-\\nstroying the features that classifiers depend on (Hendrycks\\n& Dietterich, 2019). A linear fit of KL divergence against\\naccuracy drop yields R2 = 0.042 (Figure 3f): aggregate di-\\nvergence does not predict robustness. What shifted matters\\nmore than how much.\\n4.4. Active Learning with Ground-Truth Uncertainty'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:57610bf)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'total_pages': 19, 'format': 'PDF 1.7', 'title': 'Beyond the Loss Curve: Scaling Laws, Active Learning, and the Limits of Learning from Exact Posteriors', 'author': 'Arian Khorasani; Nathaniel Chen; Yug D Oswal; Akshat Santhana Gopalan; Egemen Kolemen; Ravid Shwartz-Ziv', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 5}, page_content='accuracy drop yields R2 = 0.042 (Figure 3f): aggregate di-\\nvergence does not predict robustness. What shifted matters\\nmore than how much.\\n4.4. Active Learning with Ground-Truth Uncertainty\\nActive learning aims to select the most informative training\\nsamples (Sener & Savarese, 2018). On standard bench-\\nmarks, informativeness must be estimated via heuristics\\n(e.g., entropy of model predictions) (Lewis & Gale, 1994;\\nGal et al., 2017). Our oracle provides exact epistemic un-\\ncertainty, enabling a critical distinction: separating samples\\nthat are epistemically uncertain (informative) from those\\nthat are aleatorically uncertain (inherently ambiguous). On\\nstandard benchmarks, a high-entropy sample could be either;\\nthe oracle tells us which.\\nWe compare three acquisition strategies on ImageNet-64:\\nRandom, Max Entropy, and Max Epistemic (selecting by\\nexact epistemic uncertainty). Max Epistemic achieves a\\n1.6× larger accuracy gain than Max Entropy under the same'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:57610bf)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'total_pages': 19, 'format': 'PDF 1.7', 'title': 'Beyond the Loss Curve: Scaling Laws, Active Learning, and the Limits of Learning from Exact Posteriors', 'author': 'Arian Khorasani; Nathaniel Chen; Yug D Oswal; Akshat Santhana Gopalan; Egemen Kolemen; Ravid Shwartz-Ziv', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 5}, page_content='Random, Max Entropy, and Max Epistemic (selecting by\\nexact epistemic uncertainty). Max Epistemic achieves a\\n1.6× larger accuracy gain than Max Entropy under the same\\nlabeling budget, and requires 47.8% fewer labeled queries\\nto reach the same mid-regime performance. Max Epistemic\\nconsistently outperforms both baselines, confirming that\\npredictive entropy is a poor proxy for information gain when\\naleatoric noise is substantial. Full results and learning curves\\nappear in Appendix (Figure 9).\\n6'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:57610bf)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'total_pages': 19, 'format': 'PDF 1.7', 'title': 'Beyond the Loss Curve: Scaling Laws, Active Learning, and the Limits of Learning from Exact Posteriors', 'author': 'Arian Khorasani; Nathaniel Chen; Yug D Oswal; Akshat Santhana Gopalan; Egemen Kolemen; Ravid Shwartz-Ziv', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 6}, page_content='Decomposing Neural Network Error via Flow-Based Oracles\\nFigure 3. What shifts matters more than how much: KL magnitude alone poorly predicts performance (R2=0.04). (a) Test accuracy\\nvs. exact KL divergence: class imbalance (blue) maintains ∼97% accuracy; Gaussian noise (low points) collapses to ∼77% at comparable\\nKL. (b) Accuracy drop vs. KL: imbalance (red diamonds) clusters near zero regardless of divergence magnitude. (c) Prior shift alone\\nbarely affects accuracy (97.4–97.7% across all imbalance levels). (d) Covariate shift causes exponential degradation (σ=0.15 →77%).\\n(e) Aggregated comparison confirms noise dominates. (f) Linear regression: R2 = 0.04 shows aggregate KL is uninformative.\\n5. Discussion\\nImplications for practice.\\nFirst, validation loss is a poor\\nproxy for learning progress: epistemic error continues to\\nshrink long after loss curves flatten. Practitioners using\\nearly stopping may be leaving performance on the table.'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:57610bf)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'total_pages': 19, 'format': 'PDF 1.7', 'title': 'Beyond the Loss Curve: Scaling Laws, Active Learning, and the Limits of Learning from Exact Posteriors', 'author': 'Arian Khorasani; Nathaniel Chen; Yug D Oswal; Akshat Santhana Gopalan; Egemen Kolemen; Ravid Shwartz-Ziv', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 6}, page_content='proxy for learning progress: epistemic error continues to\\nshrink long after loss curves flatten. Practitioners using\\nearly stopping may be leaving performance on the table.\\nTracking epistemic proxies (e.g., ensemble disagreement) is\\na better strategy, though in real settings these must be esti-\\nmated (Lakshminarayanan et al., 2017; Gal & Ghahramani,\\n2016).\\nSecond, in small-data regimes without pretraining, convo-\\nlutional architectures are the better choice: ResNets reduce\\nepistemic error by 6× from N=100 to N=40K, while ViTs\\nimprove by less than 1.2×.\\nThird, the soft label results suggest that access to posterior\\ninformation, even approximate, can meaningfully improve\\nclassification. This connects to knowledge distillation (Hin-\\nton et al., 2015): a strong teacher’s soft outputs contain struc-\\nture beyond hard labels. Our oracle provides a controlled\\nsetting to study this phenomenon with exact posteriors.\\nFourth, a shift in p(x|y) (feature corruption) is far more'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:57610bf)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'total_pages': 19, 'format': 'PDF 1.7', 'title': 'Beyond the Loss Curve: Scaling Laws, Active Learning, and the Limits of Learning from Exact Posteriors', 'author': 'Arian Khorasani; Nathaniel Chen; Yug D Oswal; Akshat Santhana Gopalan; Egemen Kolemen; Ravid Shwartz-Ziv', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 6}, page_content='ture beyond hard labels. Our oracle provides a controlled\\nsetting to study this phenomenon with exact posteriors.\\nFourth, a shift in p(x|y) (feature corruption) is far more\\ndamaging than a shift in p(y) (prior change) (Hendrycks\\n& Dietterich, 2019; Chawla et al., 2002) at comparable KL\\nvalues. Robustness evaluations should characterize what\\nshifted, not just how much.\\nLimitations.\\nOur oracle defines a synthetic world, not\\nnature. Findings transfer to natural images only insofar as\\nthe flow captures relevant statistical structure. Key concerns:\\n(1) domain gap between flow samples and real photographs;\\n(2) scale limitations (AFHQ has 3 classes; full ImageNet\\nexperiments ongoing); (3) architectural conclusions may\\ndepend on hyperparameters. We view this framework as\\ncomplementary to standard benchmarks: it provides ground\\ntruth within a controlled setting, not claims about nature’s\\ntrue Bayes error.\\n6. Related Work\\nNeural scaling laws.\\nKaplan et al. (2020) established that'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:57610bf)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'total_pages': 19, 'format': 'PDF 1.7', 'title': 'Beyond the Loss Curve: Scaling Laws, Active Learning, and the Limits of Learning from Exact Posteriors', 'author': 'Arian Khorasani; Nathaniel Chen; Yug D Oswal; Akshat Santhana Gopalan; Egemen Kolemen; Ravid Shwartz-Ziv', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 6}, page_content='truth within a controlled setting, not claims about nature’s\\ntrue Bayes error.\\n6. Related Work\\nNeural scaling laws.\\nKaplan et al. (2020) established that\\nloss decreases as a power law in data, compute, and parame-\\nters. Subsequent work refined these laws for vision (Zhai\\net al., 2022) and explored their limits. All such studies mea-\\nsure total loss, conflating aleatoric and epistemic error. Our\\ndecomposition reveals that epistemic error alone follows a\\npower law, even when total loss appears flat.\\nUncertainty estimation.\\nBayesian deep learning and en-\\nsemble methods aim to quantify epistemic uncertainty, but\\nwithout ground truth, evaluation is indirect (Alemi et al.,\\n7'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:57610bf)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'total_pages': 19, 'format': 'PDF 1.7', 'title': 'Beyond the Loss Curve: Scaling Laws, Active Learning, and the Limits of Learning from Exact Posteriors', 'author': 'Arian Khorasani; Nathaniel Chen; Yug D Oswal; Akshat Santhana Gopalan; Egemen Kolemen; Ravid Shwartz-Ziv', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 7}, page_content='Decomposing Neural Network Error via Flow-Based Oracles\\n2019). Calibration metrics compare to empirical frequen-\\ncies, not true posteriors (Guo et al., 2017). Our framework\\nprovides the missing ground truth, enabling evaluation of\\nboth calibration and posterior quality.\\nKnowledge distillation and soft labels.\\nHinton et al.\\n(2015) showed that training on a teacher’s soft predictions\\ntransfers “dark knowledge” beyond hard labels. Our soft-\\nlabel experiments connect to this literature but with a key\\ndifference: our soft labels are exact posteriors, not approx-\\nimations from a teacher model. This provides an upper\\nbound on what soft-label training can achieve.\\nDistribution\\nshift.\\nBenchmarks\\nlike\\nImageNet-\\nC (Hendrycks & Dietterich, 2019) evaluate robustness to\\ndistribution shift, but the magnitude of shift is unknown.\\nOur oracle enables controlled experiments with exact\\ndivergence, revealing that shift type matters more than\\nmagnitude.\\nActive learning.\\nUncertainty-based acquisition func-'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:57610bf)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'total_pages': 19, 'format': 'PDF 1.7', 'title': 'Beyond the Loss Curve: Scaling Laws, Active Learning, and the Limits of Learning from Exact Posteriors', 'author': 'Arian Khorasani; Nathaniel Chen; Yug D Oswal; Akshat Santhana Gopalan; Egemen Kolemen; Ravid Shwartz-Ziv', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 7}, page_content='Our oracle enables controlled experiments with exact\\ndivergence, revealing that shift type matters more than\\nmagnitude.\\nActive learning.\\nUncertainty-based acquisition func-\\ntions (Gal et al., 2017; Kirsch et al., 2019) select samples by\\nestimated informativeness. Our oracle separates epistemic\\nfrom aleatoric uncertainty exactly, providing an upper bound\\non what uncertainty-based active learning can achieve.\\nNormalizing flows.\\nFlows have matured from Real-\\nNVP (Dinh et al., 2017) and Glow (Kingma & Dhariwal,\\n2018) to TarFlow (Zhai et al., 2025), which achieves strong\\nlikelihoods on ImageNet. We use flows differently: not\\nas models to evaluate, but as oracles that define evaluation\\nitself.\\nSynthetic benchmarks.\\nResearchers have long used toy\\ndistributions (Gaussians, moons, spirals) with known poste-\\nriors. These lack realism. Our approach combines realistic\\nimages from AFHQ and ImageNet with the exact posterior\\naccess of synthetic benchmarks.\\n7. Conclusion'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:57610bf)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'total_pages': 19, 'format': 'PDF 1.7', 'title': 'Beyond the Loss Curve: Scaling Laws, Active Learning, and the Limits of Learning from Exact Posteriors', 'author': 'Arian Khorasani; Nathaniel Chen; Yug D Oswal; Akshat Santhana Gopalan; Egemen Kolemen; Ravid Shwartz-Ziv', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 7}, page_content='riors. These lack realism. Our approach combines realistic\\nimages from AFHQ and ImageNet with the exact posterior\\naccess of synthetic benchmarks.\\n7. Conclusion\\nWe introduced a framework that treats normalizing flows\\nas oracles rather than models, enabling exact computation\\nof quantities that standard benchmarks can only estimate.\\nWhen we know the data-generating process, we can directly\\nmeasure what classifiers learn versus what remains funda-\\nmentally uncertain.\\nOur experiments reveal that epistemic error follows a clean\\npower law (KL ∝N −α) even when total loss plateaus:\\nmodels keep learning, but standard metrics miss it. This\\npower-law decay holds across architectures and scales to Im-\\nageNet with 1000 classes. The decomposition also reveals\\nmarked architectural differences: ResNets reduce epistemic\\nerror by 6× over the data range where ViTs improve by\\nonly 1.2×, suggesting convolutional inductive biases matter\\nmost in low-data regimes.'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:57610bf)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'total_pages': 19, 'format': 'PDF 1.7', 'title': 'Beyond the Loss Curve: Scaling Laws, Active Learning, and the Limits of Learning from Exact Posteriors', 'author': 'Arian Khorasani; Nathaniel Chen; Yug D Oswal; Akshat Santhana Gopalan; Egemen Kolemen; Ravid Shwartz-Ziv', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 7}, page_content='marked architectural differences: ResNets reduce epistemic\\nerror by 6× over the data range where ViTs improve by\\nonly 1.2×, suggesting convolutional inductive biases matter\\nmost in low-data regimes.\\nOur distribution shift experiments show that what shifts\\nmatters more than how much: class imbalance barely\\naffects accuracy at KL values where input noise causes\\n20-point drops. This finding implies that robustness eval-\\nuations should characterize the nature of shift, not just its\\nmagnitude.\\nThe oracle framework is not a replacement for real-world\\nbenchmarks; it complements them by providing ground\\ntruth within a controlled setting. This approach can inform\\narchitecture selection, training decisions, and robustness\\nevaluation in ways that aggregate loss curves cannot. Code\\nand oracle models will be made available soon.\\nImpact Statement\\nThis paper presents work whose goal is to advance the field\\nof Machine Learning by providing benchmarks with ex-\\nact information-theoretic ground truth.'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:57610bf)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'total_pages': 19, 'format': 'PDF 1.7', 'title': 'Beyond the Loss Curve: Scaling Laws, Active Learning, and the Limits of Learning from Exact Posteriors', 'author': 'Arian Khorasani; Nathaniel Chen; Yug D Oswal; Akshat Santhana Gopalan; Egemen Kolemen; Ravid Shwartz-Ziv', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 7}, page_content='Impact Statement\\nThis paper presents work whose goal is to advance the field\\nof Machine Learning by providing benchmarks with ex-\\nact information-theoretic ground truth.\\nOur framework\\nenables more rigorous evaluation of learning algorithms,\\nwhich should lead to better scientific understanding of deep\\nlearning. The oracle datasets we generate are synthetic and\\ndo not raise privacy concerns. There are many potential so-\\ncietal consequences of our work, none which we feel must\\nbe specifically highlighted here.\\nAcknowledgments\\nThis research was supported in part by the Digital Research\\nAlliance of Canada (DRAC), Calcul Qu´ebec, and the Prince-\\nton Laboratory for Artificial Intelligence, under Award No.\\n2025-97. We also acknowledge Vellore Institute of Tech-\\nnology (VIT) for providing access to eight NVIDIA V100\\nGPUs, which supported the computational experiments re-\\nported in this work.\\n8. Contributions\\nArian organized, planned, and led the project; developed'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:57610bf)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'total_pages': 19, 'format': 'PDF 1.7', 'title': 'Beyond the Loss Curve: Scaling Laws, Active Learning, and the Limits of Learning from Exact Posteriors', 'author': 'Arian Khorasani; Nathaniel Chen; Yug D Oswal; Akshat Santhana Gopalan; Egemen Kolemen; Ravid Shwartz-Ziv', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 7}, page_content='GPUs, which supported the computational experiments re-\\nported in this work.\\n8. Contributions\\nArian organized, planned, and led the project; developed\\nand implemented the core idea and codebase; refined the\\narchitecture by improving key components and training\\nstrategies; iterated on the dataset; and ran and iterated on all\\nexperiments.\\nNathaniel integrated all the experiments for the ImageNet-\\n128 dataset and contributed meaningfully to writing the\\npaper.\\nYug integrated all the experiments for the ImageNet-64\\ndataset and contributed meaningfully to writing the paper.\\nAkshat contributed to the writing of the paper.\\n8'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:57610bf)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'total_pages': 19, 'format': 'PDF 1.7', 'title': 'Beyond the Loss Curve: Scaling Laws, Active Learning, and the Limits of Learning from Exact Posteriors', 'author': 'Arian Khorasani; Nathaniel Chen; Yug D Oswal; Akshat Santhana Gopalan; Egemen Kolemen; Ravid Shwartz-Ziv', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 8}, page_content='Decomposing Neural Network Error via Flow-Based Oracles\\nEgemen Advised the project in high-level.\\nRavid advised the whole project from scratch, with feed-\\nback in all the stages, contributing to the writing of the paper,\\nand conceiving and pushing forward the research direction\\nin the all stages of the project.\\nReferences\\nAlemi, A. A., Fischer, I., Dillon, J. V., and Murphy,\\nK.\\nDeep Variational Information Bottleneck, Octo-\\nber 2019.\\nURL http://arxiv.org/abs/1612.\\n00410. arXiv:1612.00410 [cs].\\nBelghazi, M. I., Baratin, A., Rajeswar, S., Ozair, S., Bengio,\\nY., Courville, A., and Hjelm, R. D. MINE: Mutual Infor-\\nmation Neural Estimation, August 2021. URL http://\\narxiv.org/abs/1801.04062. arXiv:1801.04062\\n[cs].\\nChawla, N. V., Bowyer, K. W., Hall, L. O., and Kegelmeyer,\\nW. P. SMOTE: Synthetic Minority Over-sampling Tech-\\nnique. Journal of Artificial Intelligence Research, 16:\\n321–357, June 2002. ISSN 1076-9757. doi: 10.1613/jair.\\n953. URL http://arxiv.org/abs/1106.1813.'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:57610bf)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'total_pages': 19, 'format': 'PDF 1.7', 'title': 'Beyond the Loss Curve: Scaling Laws, Active Learning, and the Limits of Learning from Exact Posteriors', 'author': 'Arian Khorasani; Nathaniel Chen; Yug D Oswal; Akshat Santhana Gopalan; Egemen Kolemen; Ravid Shwartz-Ziv', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 8}, page_content='nique. Journal of Artificial Intelligence Research, 16:\\n321–357, June 2002. ISSN 1076-9757. doi: 10.1613/jair.\\n953. URL http://arxiv.org/abs/1106.1813.\\narXiv:1106.1813 [cs].\\nChoi, Y., Uh, Y., Yoo, J., and Ha, J.-W.\\nStarGAN\\nv2: Diverse Image Synthesis for Multiple Domains,\\nApril 2020. URL http://arxiv.org/abs/1912.\\n01865. arXiv:1912.01865 [cs].\\nChrabaszcz, P., Loshchilov, I., and Hutter, F. A Downsam-\\npled Variant of ImageNet as an Alternative to the CIFAR\\ndatasets, August 2017. URL http://arxiv.org/\\nabs/1707.08819. arXiv:1707.08819 [cs].\\nDeng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei,\\nL. ImageNet: A large-scale hierarchical image database.\\nIn 2009 IEEE Conference on Computer Vision and Pat-\\ntern Recognition, pp. 248–255, June 2009. doi: 10.1109/\\nCVPR.2009.5206848. URL https://ieeexplore.\\nieee.org/document/5206848.\\nISSN: 1063-\\n6919.\\nDinh, L., Sohl-Dickstein, J., and Bengio, S. Density esti-\\nmation using Real NVP, February 2017. URL http://'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:57610bf)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'total_pages': 19, 'format': 'PDF 1.7', 'title': 'Beyond the Loss Curve: Scaling Laws, Active Learning, and the Limits of Learning from Exact Posteriors', 'author': 'Arian Khorasani; Nathaniel Chen; Yug D Oswal; Akshat Santhana Gopalan; Egemen Kolemen; Ravid Shwartz-Ziv', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 8}, page_content='CVPR.2009.5206848. URL https://ieeexplore.\\nieee.org/document/5206848.\\nISSN: 1063-\\n6919.\\nDinh, L., Sohl-Dickstein, J., and Bengio, S. Density esti-\\nmation using Real NVP, February 2017. URL http://\\narxiv.org/abs/1605.08803. arXiv:1605.08803\\n[cs].\\nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn,\\nD., Zhai, X., Unterthiner, T., Dehghani, M., Minderer,\\nM., Heigold, G., Gelly, S., Uszkoreit, J., and Houlsby,\\nN. An Image is Worth 16x16 Words: Transformers for\\nImage Recognition at Scale, June 2021. URL http://\\narxiv.org/abs/2010.11929. arXiv:2010.11929\\n[cs].\\nGal, Y. and Ghahramani, Z. Dropout as a Bayesian Approx-\\nimation: Representing Model Uncertainty in Deep Learn-\\ning. In Proceedings of The 33rd International Conference\\non Machine Learning, pp. 1050–1059. PMLR, June 2016.\\nURL https://proceedings.mlr.press/v48/\\ngal16.html.\\nGal,\\nY.,\\nIslam,\\nR.,\\nand Ghahramani,\\nZ.\\nDeep\\nBayesian Active Learning with Image Data, March\\n2017.\\nURL\\nhttp://arxiv.org/abs/1703.\\n02910. arXiv:1703.02910 [cs].'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:57610bf)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'total_pages': 19, 'format': 'PDF 1.7', 'title': 'Beyond the Loss Curve: Scaling Laws, Active Learning, and the Limits of Learning from Exact Posteriors', 'author': 'Arian Khorasani; Nathaniel Chen; Yug D Oswal; Akshat Santhana Gopalan; Egemen Kolemen; Ravid Shwartz-Ziv', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 8}, page_content='gal16.html.\\nGal,\\nY.,\\nIslam,\\nR.,\\nand Ghahramani,\\nZ.\\nDeep\\nBayesian Active Learning with Image Data, March\\n2017.\\nURL\\nhttp://arxiv.org/abs/1703.\\n02910. arXiv:1703.02910 [cs].\\nGuo, C., Pleiss, G., Sun, Y., and Weinberger, K. Q.\\nOn Calibration of Modern Neural Networks, Au-\\ngust 2017. URL http://arxiv.org/abs/1706.\\n04599. arXiv:1706.04599 [cs].\\nHe, K., Zhang, X., Ren, S., and Sun, J.\\nDeep\\nResidual Learning for Image Recognition, Decem-\\nber 2015.\\nURL http://arxiv.org/abs/1512.\\n03385. arXiv:1512.03385 [cs].\\nHendrycks, D. and Dietterich, T. Benchmarking Neural\\nNetwork Robustness to Common Corruptions and Per-\\nturbations, March 2019. URL http://arxiv.org/\\nabs/1903.12261. arXiv:1903.12261 [cs].\\nHeusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., and\\nHochreiter, S. GANs Trained by a Two Time-Scale Up-\\ndate Rule Converge to a Local Nash Equilibrium, Jan-\\nuary 2018. URL http://arxiv.org/abs/1706.\\n08500. arXiv:1706.08500 [cs].\\nHinton, G., Vinyals, O., and Dean, J. Distilling the Knowl-'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:57610bf)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'total_pages': 19, 'format': 'PDF 1.7', 'title': 'Beyond the Loss Curve: Scaling Laws, Active Learning, and the Limits of Learning from Exact Posteriors', 'author': 'Arian Khorasani; Nathaniel Chen; Yug D Oswal; Akshat Santhana Gopalan; Egemen Kolemen; Ravid Shwartz-Ziv', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 8}, page_content='date Rule Converge to a Local Nash Equilibrium, Jan-\\nuary 2018. URL http://arxiv.org/abs/1706.\\n08500. arXiv:1706.08500 [cs].\\nHinton, G., Vinyals, O., and Dean, J. Distilling the Knowl-\\nedge in a Neural Network, March 2015. URL http://\\narxiv.org/abs/1503.02531. arXiv:1503.02531\\n[stat].\\nHoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E.,\\nCai, T., Rutherford, E., Casas, D. d. L., Hendricks, L. A.,\\nWelbl, J., Clark, A., Hennigan, T., Noland, E., Millican,\\nK., Driessche, G. v. d., Damoc, B., Guy, A., Osindero,\\nS., Simonyan, K., Elsen, E., Rae, J. W., Vinyals, O., and\\nSifre, L. Training Compute-Optimal Large Language\\nModels, March 2022.\\nURL http://arxiv.org/\\nabs/2203.15556. arXiv:2203.15556 [cs].\\nHoward, A., Sandler, M., Chu, G., Chen, L.-C., Chen, B.,\\nTan, M., Wang, W., Zhu, Y., Pang, R., Vasudevan, V.,\\nLe, Q. V., and Adam, H. Searching for MobileNetV3,\\nNovember 2019. URL http://arxiv.org/abs/\\n1905.02244. arXiv:1905.02244 [cs].\\nKaplan, J., McCandlish, S., Henighan, T., Brown, T. B.,'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:57610bf)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'total_pages': 19, 'format': 'PDF 1.7', 'title': 'Beyond the Loss Curve: Scaling Laws, Active Learning, and the Limits of Learning from Exact Posteriors', 'author': 'Arian Khorasani; Nathaniel Chen; Yug D Oswal; Akshat Santhana Gopalan; Egemen Kolemen; Ravid Shwartz-Ziv', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 8}, page_content='Le, Q. V., and Adam, H. Searching for MobileNetV3,\\nNovember 2019. URL http://arxiv.org/abs/\\n1905.02244. arXiv:1905.02244 [cs].\\nKaplan, J., McCandlish, S., Henighan, T., Brown, T. B.,\\nChess, B., Child, R., Gray, S., Radford, A., Wu, J., and\\nAmodei, D. Scaling Laws for Neural Language Mod-\\nels, January 2020. URL http://arxiv.org/abs/\\n2001.08361. arXiv:2001.08361 [cs].\\n9'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:57610bf)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'total_pages': 19, 'format': 'PDF 1.7', 'title': 'Beyond the Loss Curve: Scaling Laws, Active Learning, and the Limits of Learning from Exact Posteriors', 'author': 'Arian Khorasani; Nathaniel Chen; Yug D Oswal; Akshat Santhana Gopalan; Egemen Kolemen; Ravid Shwartz-Ziv', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 9}, page_content='Decomposing Neural Network Error via Flow-Based Oracles\\nKingma, D. P. and Dhariwal, P. Glow: Generative Flow with\\nInvertible 1x1 Convolutions, July 2018. URL http://\\narxiv.org/abs/1807.03039. arXiv:1807.03039\\n[stat].\\nKirsch, A., Amersfoort, J. v., and Gal, Y. BatchBALD: Effi-\\ncient and Diverse Batch Acquisition for Deep Bayesian\\nActive Learning, October 2019. URL http://arxiv.\\norg/abs/1906.08158. arXiv:1906.08158 [cs].\\nLakshminarayanan, B., Pritzel, A., and Blundell, C. Sim-\\nple and Scalable Predictive Uncertainty Estimation us-\\ning Deep Ensembles, November 2017. URL http://\\narxiv.org/abs/1612.01474. arXiv:1612.01474\\n[stat].\\nLewis, D. D. and Gale, W. A. A Sequential Algorithm for\\nTraining Text Classifiers, July 1994. URL http://\\narxiv.org/abs/cmp-lg/9407020.\\narXiv:cmp-\\nlg/9407020.\\nLiu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z.,\\nLin, S., and Guo, B.\\nSwin Transformer: Hierarchi-\\ncal Vision Transformer using Shifted Windows, Au-\\ngust 2021. URL http://arxiv.org/abs/2103.'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:57610bf)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'total_pages': 19, 'format': 'PDF 1.7', 'title': 'Beyond the Loss Curve: Scaling Laws, Active Learning, and the Limits of Learning from Exact Posteriors', 'author': 'Arian Khorasani; Nathaniel Chen; Yug D Oswal; Akshat Santhana Gopalan; Egemen Kolemen; Ravid Shwartz-Ziv', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 9}, page_content='Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z.,\\nLin, S., and Guo, B.\\nSwin Transformer: Hierarchi-\\ncal Vision Transformer using Shifted Windows, Au-\\ngust 2021. URL http://arxiv.org/abs/2103.\\n14030. arXiv:2103.14030 [cs].\\nLiu, Z., Mao, H., Wu, C.-Y., Feichtenhofer, C., Dar-\\nrell, T., and Xie, S.\\nA ConvNet for the 2020s,\\nMarch 2022.\\nURL http://arxiv.org/abs/\\n2201.03545. arXiv:2201.03545 [cs].\\nPapamakarios, G., Nalisnick, E., Rezende, D. J., Mo-\\nhamed, S., and Lakshminarayanan, B.\\nNormaliz-\\ning Flows for Probabilistic Modeling and Inference,\\nApril 2021. URL http://arxiv.org/abs/1912.\\n02762. arXiv:1912.02762 [stat].\\nSalimans, T., Goodfellow, I., Zaremba, W., Cheung, V., Rad-\\nford, A., and Chen, X. Improved Techniques for Training\\nGANs, June 2016. URL http://arxiv.org/abs/\\n1606.03498. arXiv:1606.03498 [cs].\\nSener, O. and Savarese, S.\\nActive Learning for Con-\\nvolutional Neural Networks:\\nA Core-Set Approach,\\nJune 2018. URL http://arxiv.org/abs/1708.\\n00489. arXiv:1708.00489 [stat].'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:57610bf)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'total_pages': 19, 'format': 'PDF 1.7', 'title': 'Beyond the Loss Curve: Scaling Laws, Active Learning, and the Limits of Learning from Exact Posteriors', 'author': 'Arian Khorasani; Nathaniel Chen; Yug D Oswal; Akshat Santhana Gopalan; Egemen Kolemen; Ravid Shwartz-Ziv', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 9}, page_content='Sener, O. and Savarese, S.\\nActive Learning for Con-\\nvolutional Neural Networks:\\nA Core-Set Approach,\\nJune 2018. URL http://arxiv.org/abs/1708.\\n00489. arXiv:1708.00489 [stat].\\nSzegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., and Wojna,\\nZ. Rethinking the Inception Architecture for Computer\\nVision, December 2015. URL http://arxiv.org/\\nabs/1512.00567. arXiv:1512.00567 [cs].\\nZhai, S., Zhang, R., Nakkiran, P., Berthelot, D., Gu, J.,\\nZheng, H., Chen, T., Bautista, M. A., Jaitly, N., and\\nSusskind, J. Normalizing Flows are Capable Generative\\nModels, June 2025. URL http://arxiv.org/abs/\\n2412.06329. arXiv:2412.06329 [cs].\\nZhai, X., Kolesnikov, A., Houlsby, N., and Beyer, L. Scaling\\nVision Transformers, June 2022. URL http://arxiv.\\norg/abs/2106.04560. arXiv:2106.04560 [cs].\\n10'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:57610bf)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'total_pages': 19, 'format': 'PDF 1.7', 'title': 'Beyond the Loss Curve: Scaling Laws, Active Learning, and the Limits of Learning from Exact Posteriors', 'author': 'Arian Khorasani; Nathaniel Chen; Yug D Oswal; Akshat Santhana Gopalan; Egemen Kolemen; Ravid Shwartz-Ziv', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 10}, page_content='Decomposing Neural Network Error via Flow-Based Oracles\\nA. Oracle Validation Details\\nWe validated the oracle along five axes on both AFHQ and ImageNet-64. This appendix provides full details for the summary\\nin Section 2. We report quantitative checks verifying our Oracle provides (i) high-quality samples, (ii) broad support over\\nthe data manifold, and (iii) stable posteriors for uncertainty decomposition.\\nA.1. Distribution Quality (FID)\\nWe measured Fr´echet Inception Distance (FID) (Heusel et al., 2018), which compares the mean and covariance of Inception-\\nv3 features between real and generated images. Our AFHQ oracle achieves FID 28.44 and ImageNet-64 achieves FID 13.48,\\ncomparable to state-of-the-art generative models on these datasets.\\nA.2. Manifold Coverage\\nA generative model might produce high-quality samples that only capture part of the true distribution (e.g., generating'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:57610bf)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'total_pages': 19, 'format': 'PDF 1.7', 'title': 'Beyond the Loss Curve: Scaling Laws, Active Learning, and the Limits of Learning from Exact Posteriors', 'author': 'Arian Khorasani; Nathaniel Chen; Yug D Oswal; Akshat Santhana Gopalan; Egemen Kolemen; Ravid Shwartz-Ziv', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 10}, page_content='A.2. Manifold Coverage\\nA generative model might produce high-quality samples that only capture part of the true distribution (e.g., generating\\nrealistic dogs but missing rare breeds). We computed manifold coverage by embedding both real and generated samples in a\\npretrained feature space and measuring what fraction of real samples have a nearby generated sample within a distance\\nthreshold. In our implementation, distances are computed in ResNet feature space using k-nearest neighbors and an adaptive\\nthreshold (set to the 90th percentile of real-to-synthetic distances). Our oracle achieves 90% coverage on AFHQ and 89.9%\\non ImageNet, suggesting it captures broad distributional structure rather than collapsing to a few high-density modes.\\nA.3. Diversity (Inception Score)\\nWe evaluated diversity using Inception Score (IS), which measures the KL divergence between the conditional label distribu-'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:57610bf)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'total_pages': 19, 'format': 'PDF 1.7', 'title': 'Beyond the Loss Curve: Scaling Laws, Active Learning, and the Limits of Learning from Exact Posteriors', 'author': 'Arian Khorasani; Nathaniel Chen; Yug D Oswal; Akshat Santhana Gopalan; Egemen Kolemen; Ravid Shwartz-Ziv', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 10}, page_content='A.3. Diversity (Inception Score)\\nWe evaluated diversity using Inception Score (IS), which measures the KL divergence between the conditional label distribu-\\ntion p(y|x) and the marginal p(y) averaged over generated samples. Higher scores indicate both confident classifications\\nand diverse outputs. We achieve 6.24 ± 3.57 on AFHQ and 32.57 ± 4.47 on ImageNet. Per-class diversity ratios (variance\\nratio and distance ratio between synthetic and real) range from 0.75 to 0.96 on AFHQ, confirming reasonable diversity that\\nis slightly below real data. Across the three classes, the synthetic-to-real feature variance ratios are {0.746, 0.854, 0.921}\\nand the average pairwise distance ratios are {0.891, 0.944, 0.965}, indicating slightly reduced diversity relative to real data,\\nbut without severe collapse\\nA.4. Semantic Feature Alignment\\nPixel-level metrics do not capture whether images have the right semantic content. We extracted features from a pretrained'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:57610bf)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'total_pages': 19, 'format': 'PDF 1.7', 'title': 'Beyond the Loss Curve: Scaling Laws, Active Learning, and the Limits of Learning from Exact Posteriors', 'author': 'Arian Khorasani; Nathaniel Chen; Yug D Oswal; Akshat Santhana Gopalan; Egemen Kolemen; Ravid Shwartz-Ziv', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 10}, page_content='but without severe collapse\\nA.4. Semantic Feature Alignment\\nPixel-level metrics do not capture whether images have the right semantic content. We extracted features from a pretrained\\nResNet, which encode high-level structure like shape and pose, and compared statistics between real and generated images.\\nFeature variance matches at 83–92% across classes on AFHQ, with Class 0 (cat) performing best (92%) and Classes 1–2\\n(dog, wild) showing larger distributional separation but still acceptable overlap. ImageNet-64 results show 72–93% feature\\nvariance match across classes.\\nA.5. Memorization Check\\nIf the flow memorizes training images, our benchmark would be meaningless. We computed nearest-neighbor distances\\nin Inception feature space between each generated image and the training set. 36% of generated samples on AFHQ have\\na training neighbor within distance 10 in feature space; 6.53% on ImageNet. This thresholded “near-neighbor” rate is a'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:57610bf)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'total_pages': 19, 'format': 'PDF 1.7', 'title': 'Beyond the Loss Curve: Scaling Laws, Active Learning, and the Limits of Learning from Exact Posteriors', 'author': 'Arian Khorasani; Nathaniel Chen; Yug D Oswal; Akshat Santhana Gopalan; Egemen Kolemen; Ravid Shwartz-Ziv', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 10}, page_content='a training neighbor within distance 10 in feature space; 6.53% on ImageNet. This thresholded “near-neighbor” rate is a\\nconservative proxy for potential memorization rather than direct evidence of exact duplication. Visual inspection confirms\\nthat “close” pairs share high-level attributes (pose, lighting) but depict different individuals. The nearest-neighbor distance\\ndistribution is well-separated from zero, with the memorization threshold clearly above the bulk of the distribution; (See\\nFigure 4).\\nA.6. Texture Analysis\\nGenerated images are slightly smoother than real photographs: pixel-level texture variance is 56–69% of real images across\\nall classes. This is a known property of flow-based models, which tend to soften fine-grained details while preserving global\\nstructure. For our purposes, this is acceptable because our downstream classifiers primarily rely on semantic cues (shape,'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:57610bf)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'total_pages': 19, 'format': 'PDF 1.7', 'title': 'Beyond the Loss Curve: Scaling Laws, Active Learning, and the Limits of Learning from Exact Posteriors', 'author': 'Arian Khorasani; Nathaniel Chen; Yug D Oswal; Akshat Santhana Gopalan; Egemen Kolemen; Ravid Shwartz-Ziv', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 10}, page_content='structure. For our purposes, this is acceptable because our downstream classifiers primarily rely on semantic cues (shape,\\npose, object parts) rather than fine-grained pixel texture. Since this smoothing effect is systematic across the oracle-generated\\ndataset, comparative analyses across architectures remain valid.\\n11'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:57610bf)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'total_pages': 19, 'format': 'PDF 1.7', 'title': 'Beyond the Loss Curve: Scaling Laws, Active Learning, and the Limits of Learning from Exact Posteriors', 'author': 'Arian Khorasani; Nathaniel Chen; Yug D Oswal; Akshat Santhana Gopalan; Egemen Kolemen; Ravid Shwartz-Ziv', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 11}, page_content='Decomposing Neural Network Error via Flow-Based Oracles\\nDataset\\nFID↓\\nIS↑\\nCoverage↑\\nMem. rate\\nNN dist. (med)\\nPost. stab. KL↓\\nAFHQ-256\\n28.44\\n6.24 ± 3.57\\n0.900\\n0.363\\n11.01\\n1.51 × 10−9\\nTable 3. Oracle passes all validation checks: high coverage, low memorization, stable posteriors. FID measures distributional\\nquality; coverage confirms broad manifold support (90%); memorization rate shows most samples are novel; posterior stability KL (10−9)\\nconfirms robustness to small perturbations.\\nTable 4. Oracle samples match real data statistics across multiple metrics. FID, Inception Score, and manifold coverage confirm\\ndistributional quality; low memorization rates verify sample novelty. Dashes indicate metrics not yet computed.\\nMetric\\nAFHQ\\nImageNet-64\\nFID ↓\\n28.44\\n13.48\\nInception Score ↑\\n6.24 ± 3.57\\n32.57 ± 4.47\\nManifold coverage\\n90%\\n89.9%\\nFeature variance match\\n83–92%\\n72–93%\\nTexture variance ratio\\n56–69%\\n—\\nFeature variance ratio match\\n—\\n71.6–93.3%\\nMemorization rate\\n36%\\n6.5%'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:57610bf)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'total_pages': 19, 'format': 'PDF 1.7', 'title': 'Beyond the Loss Curve: Scaling Laws, Active Learning, and the Limits of Learning from Exact Posteriors', 'author': 'Arian Khorasani; Nathaniel Chen; Yug D Oswal; Akshat Santhana Gopalan; Egemen Kolemen; Ravid Shwartz-Ziv', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 11}, page_content='6.24 ± 3.57\\n32.57 ± 4.47\\nManifold coverage\\n90%\\n89.9%\\nFeature variance match\\n83–92%\\n72–93%\\nTexture variance ratio\\n56–69%\\n—\\nFeature variance ratio match\\n—\\n71.6–93.3%\\nMemorization rate\\n36%\\n6.5%\\nFigure 4. Oracle samples are not memorized from training data. Distribution of nearest-neighbor distances (ResNet feature space)\\nbetween generated and training images. The dashed line marks the memorization threshold (d=10); the bulk of distances lie well above,\\nconfirming sample novelty.\\n12'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:57610bf)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'total_pages': 19, 'format': 'PDF 1.7', 'title': 'Beyond the Loss Curve: Scaling Laws, Active Learning, and the Limits of Learning from Exact Posteriors', 'author': 'Arian Khorasani; Nathaniel Chen; Yug D Oswal; Akshat Santhana Gopalan; Egemen Kolemen; Ravid Shwartz-Ziv', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 12}, page_content='Decomposing Neural Network Error via Flow-Based Oracles\\nB. TarFlow Architecture\\nWe train TarFlow models from scratch on AFHQ at 256×256 and class-conditional ImageNet at 64×64 and 128×128,\\nfollowing the architecture introduced by Zhai et al. (2025). All configurations use an autoregressive Transformer backbone\\nwith 8 blocks and 8 flow layers per block. For all datasets, we use the same optimizer settings (learning rate 10−4) and\\nlabel-dropout of 0.1 for class-conditional training. Input dequantization is performed by adding Gaussian noise with\\ndataset-specific standard deviation σ (Table 5). We compute and cache dataset statistics required for FID evaluation using\\nthe corresponding ground-truth training distribution at each resolution prior to sampling and evaluation.\\nTraining and evaluation protocol.\\nWe train using distributed data-parallelism with dataset-dependent GPU counts (8'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:57610bf)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'total_pages': 19, 'format': 'PDF 1.7', 'title': 'Beyond the Loss Curve: Scaling Laws, Active Learning, and the Limits of Learning from Exact Posteriors', 'author': 'Arian Khorasani; Nathaniel Chen; Yug D Oswal; Akshat Santhana Gopalan; Egemen Kolemen; Ravid Shwartz-Ziv', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 12}, page_content='Training and evaluation protocol.\\nWe train using distributed data-parallelism with dataset-dependent GPU counts (8\\nGPUs for AFHQ 2562 and ImageNet 642; 32 GPUs for ImageNet 1282). During training, we periodically generate samples\\nand evaluate FID using cached ground-truth statistics computed at the matching resolution. For conditional generation at\\nevaluation time, we additionally report classifier-free guidance (CFG) results by sampling with a nonzero guidance scale\\n(e.g., γ = 2.3 for ImageNet 642), while keeping guidance disabled during training. We emphasize that all reported ImageNet\\nand AFHQ models are trained from scratch using the above protocol.\\n13'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:57610bf)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'total_pages': 19, 'format': 'PDF 1.7', 'title': 'Beyond the Loss Curve: Scaling Laws, Active Learning, and the Limits of Learning from Exact Posteriors', 'author': 'Arian Khorasani; Nathaniel Chen; Yug D Oswal; Akshat Santhana Gopalan; Egemen Kolemen; Ravid Shwartz-Ziv', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 13}, page_content='Decomposing Neural Network Error via Flow-Based Oracles\\nHyperparameter\\nAFHQ 2562\\nImageNet 642\\nImageNet 1282\\nConditioning\\nClass-conditional\\nClass-conditional\\nClass-conditional\\nImage size\\n256\\n64\\n128\\nChannels (C)\\n768\\n768\\n1024\\nPatch size\\n8\\n2\\n4\\n# Transformer blocks\\n8\\n8\\n8\\nFlow layers / block\\n8\\n8\\n8\\nGaussian noise std (σ)\\n0.07\\n0.05\\n0.15\\nLearning rate\\n1 × 10−4\\n1 × 10−4\\n1 × 10−4\\nBatch size\\n256\\n256\\n768\\nEpochs\\n4000\\n200\\n320\\nLabel dropout (p)\\n0.1\\n0.1\\n0.1\\nCFG during training\\n0 (disabled)\\n0 (disabled)\\n0 (disabled)\\nTable 5. TarFlow hyperparameters for reproducibility. All models use 8 Transformer blocks with 8 flow layers each. Gaussian noise\\nrefers to dequantization noise during training.\\nC. Full Scaling Results\\nFigure 5. Full scaling results on AFHQ. Epistemic uncertainty follows power-law decay across all architectures, with MobileNet showing\\nthe steepest decline and ViT the shallowest.\\n14'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:57610bf)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'total_pages': 19, 'format': 'PDF 1.7', 'title': 'Beyond the Loss Curve: Scaling Laws, Active Learning, and the Limits of Learning from Exact Posteriors', 'author': 'Arian Khorasani; Nathaniel Chen; Yug D Oswal; Akshat Santhana Gopalan; Egemen Kolemen; Ravid Shwartz-Ziv', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 14}, page_content='Decomposing Neural Network Error via Flow-Based Oracles\\nFigure 6. Scaling laws extend to ImageNet-64 with 1000 classes. Power-law epistemic decay holds at scale, though MobileNet shows a\\nmid-range transition regime before resuming clean scaling.\\nD. Soft Labels: Full Results\\nOur oracle can generate exact posterior distributions p(y|x) as training labels, not just the argmax class. We compare\\ntraining with hard labels (one-hot from the most likely class) versus soft labels (the full posterior vector) across dataset sizes\\nfrom N=100 to N=5,000.\\nFigure 7 shows that training with oracle soft labels outperforms hard-label training at 4 out of 5 dataset sizes, with accuracy\\ngains up to ∼1%. The one exception is N=500, where hard labels slightly outperform.\\nThis result validates oracle quality from a different angle: the soft posteriors contain learnable information beyond the'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:57610bf)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'total_pages': 19, 'format': 'PDF 1.7', 'title': 'Beyond the Loss Curve: Scaling Laws, Active Learning, and the Limits of Learning from Exact Posteriors', 'author': 'Arian Khorasani; Nathaniel Chen; Yug D Oswal; Akshat Santhana Gopalan; Egemen Kolemen; Ravid Shwartz-Ziv', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 14}, page_content='This result validates oracle quality from a different angle: the soft posteriors contain learnable information beyond the\\nclass label. If the oracle were merely assigning noisy labels, soft training would not consistently outperform. Instead, the\\nposteriors encode genuine uncertainty structure (inter-class similarities, ambiguous regions, confidence gradients) that\\nclassifiers can exploit. This form of supervision is unavailable on any standard benchmark, where labels are always hard.\\nModels trained on soft labels also achieve near-perfect calibration (ECE = 0.018), learning the full uncertainty landscape\\nrather than just decision boundaries.\\n15'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:57610bf)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'total_pages': 19, 'format': 'PDF 1.7', 'title': 'Beyond the Loss Curve: Scaling Laws, Active Learning, and the Limits of Learning from Exact Posteriors', 'author': 'Arian Khorasani; Nathaniel Chen; Yug D Oswal; Akshat Santhana Gopalan; Egemen Kolemen; Ravid Shwartz-Ziv', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 15}, page_content='Decomposing Neural Network Error via Flow-Based Oracles\\nFigure 7. Exact soft labels outperform hard labels at most dataset sizes. Top: Training curves show soft labels (blue) tracking above\\nhard labels (red) across three dataset sizes. Bottom left: Final accuracy vs. N; soft labels win at 4 of 5 sizes. Bottom right: Accuracy\\ngain reaches ∼1%, confirming oracle posteriors encode learnable structure beyond class labels.\\nE. Full Active Learning Results\\nFigure 8. Epistemic-based acquisition reduces uncertainty fastest on AFHQ. (a) Learning curves: Max Epistemic (red) initially dips\\nwhile selecting challenging samples, then recovers. (b) Epistemic KL reduction: Max Epistemic achieves the steepest decline, confirming\\nit targets genuinely informative samples. (c) Final accuracy: all methods converge to ∼97–98% on this 3-class dataset.\\n16'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:57610bf)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'total_pages': 19, 'format': 'PDF 1.7', 'title': 'Beyond the Loss Curve: Scaling Laws, Active Learning, and the Limits of Learning from Exact Posteriors', 'author': 'Arian Khorasani; Nathaniel Chen; Yug D Oswal; Akshat Santhana Gopalan; Egemen Kolemen; Ravid Shwartz-Ziv', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 16}, page_content='Decomposing Neural Network Error via Flow-Based Oracles\\nFigure 9. Max Epistemic consistently outperforms entropy-based selection on ImageNet-64. Selecting by exact epistemic uncertainty\\n(orange) beats both random (gray) and max entropy (blue), confirming that predictive entropy conflates aleatoric and epistemic components\\nwhile our decomposition isolates the informative signal.\\nF. Computing Posteriors at Scale\\nThe bijective invertability of normalizing flow models allow us to independently calculate samples in parallel. Posteriors are\\nobtained by iteratively cycling through smaller batches of classes per batch of image across for all possible classes. Since\\nthe results can quickly accumulate, we store the outputs of the image batch until a given size, then save the combined output\\nas a shard for each GPU. For ImageNet64, this process has been split across 7 V100 GPUs for a total of 4,200 samples. And'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:57610bf)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'total_pages': 19, 'format': 'PDF 1.7', 'title': 'Beyond the Loss Curve: Scaling Laws, Active Learning, and the Limits of Learning from Exact Posteriors', 'author': 'Arian Khorasani; Nathaniel Chen; Yug D Oswal; Akshat Santhana Gopalan; Egemen Kolemen; Ravid Shwartz-Ziv', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 16}, page_content='as a shard for each GPU. For ImageNet64, this process has been split across 7 V100 GPUs for a total of 4,200 samples. And\\nfor ImageNet128, this process has been split across 4 H200 GPUs for a total of 16,800 samples. For the Imagenet datasets,\\nwe iteratively accumulate 50 class batches per image batch which we find to be a good balance for memory efficiency.\\nOne notable difference between the models is that OracleFlow using the AFHQ 256x256 model uses a smoothing temperature\\nof 500 across 3 classes, compared to ImageNet 64x64 and ImageNet 128x128 which uses a smoothing temperature of 1 due\\nto ImageNet’s much higher class size of 1000.\\nG. Distribution Shift Experimental Details\\nWe are providing the full specification of controlled distribution-shift protocol used to stress-test supervised learners trained\\non oracle-generated data. We generate training-time shifts via two orthogonal knobs: (i) label-prior shift (class imbalance)'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:57610bf)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'total_pages': 19, 'format': 'PDF 1.7', 'title': 'Beyond the Loss Curve: Scaling Laws, Active Learning, and the Limits of Learning from Exact Posteriors', 'author': 'Arian Khorasani; Nathaniel Chen; Yug D Oswal; Akshat Santhana Gopalan; Egemen Kolemen; Ravid Shwartz-Ziv', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 16}, page_content='on oracle-generated data. We generate training-time shifts via two orthogonal knobs: (i) label-prior shift (class imbalance)\\nand (ii) covariate shift via additive Gaussian noise. All models are evaluated on the same held-out baseline test set to isolate\\nthe impact of training distribution mismatch.\\nG.1. Base dataset and notation\\nLet D0 = {(xi, yi)}Npool\\ni=1 denote a fixed pool of labeled images stored in the oracle raw range x ∈[−1, 1] with K = 3 classes.\\nWe also fix a baseline test set Dtest\\n0\\n(size Ntest = 2000) sampled once from the same source and used for all evaluations.\\nBaseline label distribution.\\nWe define the baseline label prior as uniform:\\nP0(y = k) = 1\\nK\\n(K = 3).\\n(5)\\nG.2. Shift family: label-prior shift and noise perturbations\\nEach shifted training distribution is parameterized by (π, σ), where π = (π0, π1, π2) specifies the target class prior and\\nσ ≥0 controls Gaussian covariate noise.\\n(A) Label-prior (class-imbalance) shift.\\nWe sample class counts via multinomial draw:'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:57610bf)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'total_pages': 19, 'format': 'PDF 1.7', 'title': 'Beyond the Loss Curve: Scaling Laws, Active Learning, and the Limits of Learning from Exact Posteriors', 'author': 'Arian Khorasani; Nathaniel Chen; Yug D Oswal; Akshat Santhana Gopalan; Egemen Kolemen; Ravid Shwartz-Ziv', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 16}, page_content='σ ≥0 controls Gaussian covariate noise.\\n(A) Label-prior (class-imbalance) shift.\\nWe sample class counts via multinomial draw:\\n(n0, n1, n2) ∼Multinomial(Ntrain, π) ,\\n(6)\\nthen construct the shifted training set by sampling nk examples from the pool restricted to class k. If nk exceeds the\\navailable pool size for class k, we sample with replacement (this matches the implementation).\\n17'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:57610bf)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'total_pages': 19, 'format': 'PDF 1.7', 'title': 'Beyond the Loss Curve: Scaling Laws, Active Learning, and the Limits of Learning from Exact Posteriors', 'author': 'Arian Khorasani; Nathaniel Chen; Yug D Oswal; Akshat Santhana Gopalan; Egemen Kolemen; Ravid Shwartz-Ziv', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 17}, page_content='Decomposing Neural Network Error via Flow-Based Oracles\\nTable 6. Distribution-shift training protocol. All experiments use identical hyperparameters; only the training distribution varies.\\nComponent\\nSetting\\nBase architecture\\nResNet-50 pretrained on ImageNet-1K\\nClassifier head\\nReplace final FC with K = 3 outputs\\nTraining size\\nNtrain = 5000 (shifted)\\nValidation split\\n80/20 split of the shifted training set\\nTest set\\nBaseline test set Dtest\\n0 , Ntest = 2000\\nOptimizer\\nAdamW (weight decay 0.01)\\nLearning rate\\n10−4\\nSchedule\\nCosine annealing over 40 epochs\\nBatch size\\n32\\nSeeds\\nS = 3\\nMetric\\nTest accuracy on Dtest\\n0 ; error rate = 1 −acc\\n(B) Covariate shift via additive Gaussian noise.\\nGiven a sampled image x ∈[−1, 1], we generate a noised view:\\nx′ = clip (x + ε, −1, 1) ,\\nε ∼N(0, σ2I).\\n(7)\\nNoise is applied after selecting examples according to π and before normalization.\\nNormalization for training.\\nModels are trained using ImageNet-style normalization. Concretely, we map x′ ∈[−1, 1] to'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:57610bf)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'total_pages': 19, 'format': 'PDF 1.7', 'title': 'Beyond the Loss Curve: Scaling Laws, Active Learning, and the Limits of Learning from Exact Posteriors', 'author': 'Arian Khorasani; Nathaniel Chen; Yug D Oswal; Akshat Santhana Gopalan; Egemen Kolemen; Ravid Shwartz-Ziv', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 17}, page_content='Noise is applied after selecting examples according to π and before normalization.\\nNormalization for training.\\nModels are trained using ImageNet-style normalization. Concretely, we map x′ ∈[−1, 1] to\\n[0, 1] via (x′ + 1)/2 and then apply per-channel mean/std normalization.\\nG.3. KL computation methodology\\nWe report a label-marginal KL that quantifies the strength of the prior shift:\\nKLy(Pshift(y) ∥P0(y)) =\\nK\\nX\\nk=1\\nˆπk log ˆπk\\n1/K ,\\n(8)\\nwhere ˆπk is the empirical class frequency in the constructed shifted training set (i.e., computed from the realized multinomial\\nsample and any resampling-with-replacement), and log is the natural logarithm (units: nats).\\nImportant implication.\\nKLy is insensitive to pure covariate shifts induced by σ when class priors remain balanced.\\nTherefore, we always report both (σ, KLy): KLy measures label shift strength, while σ measures the covariate-noise shift\\nstrength.\\nG.4. Per-experiment protocol (fixed across all shift settings)'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:57610bf)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'total_pages': 19, 'format': 'PDF 1.7', 'title': 'Beyond the Loss Curve: Scaling Laws, Active Learning, and the Limits of Learning from Exact Posteriors', 'author': 'Arian Khorasani; Nathaniel Chen; Yug D Oswal; Akshat Santhana Gopalan; Egemen Kolemen; Ravid Shwartz-Ziv', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 17}, page_content='Therefore, we always report both (σ, KLy): KLy measures label shift strength, while σ measures the covariate-noise shift\\nstrength.\\nG.4. Per-experiment protocol (fixed across all shift settings)\\nEach shift setting (π, σ) defines one training distribution. We repeat each experiment over S = 3 random seeds (affecting\\nmultinomial draw, sampling, and optimization).\\nG.5. Shift configurations and measured KLy\\nTable 7 enumerates all perturbations. We report the target KLy implied by π (using Eq. 8 with ˆπ = π), and the empirical\\nKLy computed from realized label frequencies (mean ± std over seeds).\\nG.6. Results: performance degradation under training-time shift\\nTable 8 reports accuracy on the baseline test set Dtest\\n0\\nfor each perturbation, averaged over three seeds. We also report\\n∆TestAcc (percentage points, pp) relative to the balanced baseline, and the corresponding test error rate.\\nInterpretation (high-level).'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:57610bf)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'total_pages': 19, 'format': 'PDF 1.7', 'title': 'Beyond the Loss Curve: Scaling Laws, Active Learning, and the Limits of Learning from Exact Posteriors', 'author': 'Arian Khorasani; Nathaniel Chen; Yug D Oswal; Akshat Santhana Gopalan; Egemen Kolemen; Ravid Shwartz-Ziv', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 17}, page_content='∆TestAcc (percentage points, pp) relative to the balanced baseline, and the corresponding test error rate.\\nInterpretation (high-level).\\nAcross the tested range, label-prior shift alone (up to KLy ≈0.29 nats) causes only minor\\nchanges in baseline test accuracy (sub-0.3pp on average). In contrast, covariate noise drives substantially larger degradation,\\nwith σ = 0.10 inducing ≈2pp drops and σ = 0.15 leading to severe and high-variance failures. This separation is expected\\nbecause KLy captures label shift only, while σ controls covariate shift strength.\\n18'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:57610bf)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'total_pages': 19, 'format': 'PDF 1.7', 'title': 'Beyond the Loss Curve: Scaling Laws, Active Learning, and the Limits of Learning from Exact Posteriors', 'author': 'Arian Khorasani; Nathaniel Chen; Yug D Oswal; Akshat Santhana Gopalan; Egemen Kolemen; Ravid Shwartz-Ziv', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 18}, page_content='Decomposing Neural Network Error via Flow-Based Oracles\\nTable 7. Controlled shift configurations with exact KL values. We vary class prior π (label shift) and noise σ (covariate shift)\\nindependently. Empirical KL matches target values closely.\\nConfiguration\\nπ = (π0, π1, π2)\\nσ\\nKLy (target)\\nKLy (empirical)\\nBalanced (Baseline)\\n(0.33, 0.33, 0.34)\\n0.00\\n1.00×10−4\\n1.27×10−4 ± 1.56×10−4\\nMild Imbalance (40/35/25)\\n(0.40, 0.35, 0.25)\\n0.00\\n0.017\\n0.018 ± 0.001\\nModerate Imbalance (50/30/20)\\n(0.50, 0.30, 0.20)\\n0.00\\n0.069\\n0.070 ± 0.003\\nStrong Imbalance (60/25/15)\\n(0.60, 0.25, 0.15)\\n0.00\\n0.161\\n0.158 ± 0.007\\nVery Strong Imbalance (70/20/10)\\n(0.70, 0.20, 0.10)\\n0.00\\n0.296\\n0.293 ± 0.009\\nBalanced + Noise σ=0.05\\n(0.33, 0.33, 0.34)\\n0.05\\n1.00×10−4\\n1.27×10−4 ± 1.56×10−4\\nBalanced + Noise σ=0.10\\n(0.33, 0.33, 0.34)\\n0.10\\n1.00×10−4\\n1.27×10−4 ± 1.56×10−4\\nBalanced + Noise σ=0.15\\n(0.33, 0.33, 0.34)\\n0.15\\n1.00×10−4\\n1.27×10−4 ± 1.56×10−4\\nImbalance (50/30/20) + Noise σ=0.05\\n(0.50, 0.30, 0.20)\\n0.05\\n0.069\\n0.070 ± 0.003'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:57610bf)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'total_pages': 19, 'format': 'PDF 1.7', 'title': 'Beyond the Loss Curve: Scaling Laws, Active Learning, and the Limits of Learning from Exact Posteriors', 'author': 'Arian Khorasani; Nathaniel Chen; Yug D Oswal; Akshat Santhana Gopalan; Egemen Kolemen; Ravid Shwartz-Ziv', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 18}, page_content='0.10\\n1.00×10−4\\n1.27×10−4 ± 1.56×10−4\\nBalanced + Noise σ=0.15\\n(0.33, 0.33, 0.34)\\n0.15\\n1.00×10−4\\n1.27×10−4 ± 1.56×10−4\\nImbalance (50/30/20) + Noise σ=0.05\\n(0.50, 0.30, 0.20)\\n0.05\\n0.069\\n0.070 ± 0.003\\nImbalance (70/20/10) + Noise σ=0.10\\n(0.70, 0.20, 0.10)\\n0.10\\n0.296\\n0.293 ± 0.009\\nTable 8. Noise causes 20-point accuracy drops; imbalance causes <0.3 points. All models evaluated on the same baseline test set.\\nLabel-prior shift (up to KL=0.29) barely affects accuracy; covariate noise (σ=0.15) causes catastrophic degradation.\\nConfiguration\\nTest Acc (%)\\n∆TestAcc (pp)\\nTest Error (%)\\nVal Acc (%)\\nBalanced (Baseline)\\n97.67 ± 0.10\\n+0.00\\n2.33 ± 0.10\\n98.20 ± 0.26\\nMild Imbalance (40/35/25)\\n97.58 ± 0.10\\n-0.08\\n2.42 ± 0.10\\n98.10 ± 0.44\\nModerate Imbalance (50/30/20)\\n97.45 ± 0.13\\n-0.22\\n2.55 ± 0.13\\n98.40 ± 0.60\\nStrong Imbalance (60/25/15)\\n97.40 ± 0.09\\n-0.27\\n2.60 ± 0.09\\n98.70 ± 0.20\\nVery Strong Imbalance (70/20/10)\\n97.38 ± 0.20\\n-0.28\\n2.62 ± 0.20\\n98.80 ± 0.26\\nBalanced + Noise σ=0.05\\n97.58 ± 0.12\\n-0.08'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:57610bf)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'total_pages': 19, 'format': 'PDF 1.7', 'title': 'Beyond the Loss Curve: Scaling Laws, Active Learning, and the Limits of Learning from Exact Posteriors', 'author': 'Arian Khorasani; Nathaniel Chen; Yug D Oswal; Akshat Santhana Gopalan; Egemen Kolemen; Ravid Shwartz-Ziv', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 18}, page_content='Strong Imbalance (60/25/15)\\n97.40 ± 0.09\\n-0.27\\n2.60 ± 0.09\\n98.70 ± 0.20\\nVery Strong Imbalance (70/20/10)\\n97.38 ± 0.20\\n-0.28\\n2.62 ± 0.20\\n98.80 ± 0.26\\nBalanced + Noise σ=0.05\\n97.58 ± 0.12\\n-0.08\\n2.42 ± 0.12\\n98.13 ± 0.35\\nBalanced + Noise σ=0.10\\n95.70 ± 0.87\\n-1.97\\n4.30 ± 0.87\\n98.00 ± 0.35\\nBalanced + Noise σ=0.15\\n76.92 ± 13.02\\n-20.75\\n23.08 ± 13.02\\n98.03 ± 0.42\\nImbalance (50/30/20) + Noise σ=0.05\\n97.03 ± 0.38\\n-0.63\\n2.97 ± 0.38\\n98.80 ± 0.44\\nImbalance (70/20/10) + Noise σ=0.10\\n95.65 ± 1.03\\n-2.02\\n4.35 ± 1.03\\n99.27 ± 0.58\\nTable 9. Isolating shift axes: prior shift is benign, covariate shift is catastrophic. Left: varying class imbalance with no noise. Right:\\nvarying noise with balanced classes. The asymmetry is stark.\\nPrior shift only (σ=0)\\nNoise shift only (balanced priors)\\nConfig\\nKLy\\nTest Acc (%)\\n∆(pp)\\nσ\\nTest Acc (%)\\n∆(pp)\\nBalanced\\n≈0\\n97.67 ± 0.10\\n+0.00\\n0.05\\n97.58 ± 0.12\\n-0.08\\n40/35/25\\n0.018\\n97.58 ± 0.10\\n-0.08\\n0.10\\n95.70 ± 0.87\\n-1.97\\n50/30/20\\n0.070\\n97.45 ± 0.13\\n-0.22\\n0.15\\n76.92 ± 13.02\\n-20.75'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:57610bf)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'file_path': '..\\\\data\\\\pdf\\\\2602.00315v1.pdf', 'total_pages': 19, 'format': 'PDF 1.7', 'title': 'Beyond the Loss Curve: Scaling Laws, Active Learning, and the Limits of Learning from Exact Posteriors', 'author': 'Arian Khorasani; Nathaniel Chen; Yug D Oswal; Akshat Santhana Gopalan; Egemen Kolemen; Ravid Shwartz-Ziv', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 18}, page_content='∆(pp)\\nσ\\nTest Acc (%)\\n∆(pp)\\nBalanced\\n≈0\\n97.67 ± 0.10\\n+0.00\\n0.05\\n97.58 ± 0.12\\n-0.08\\n40/35/25\\n0.018\\n97.58 ± 0.10\\n-0.08\\n0.10\\n95.70 ± 0.87\\n-1.97\\n50/30/20\\n0.070\\n97.45 ± 0.13\\n-0.22\\n0.15\\n76.92 ± 13.02\\n-20.75\\n60/25/15\\n0.158\\n97.40 ± 0.09\\n-0.27\\n—\\n—\\n—\\n70/20/10\\n0.293\\n97.38 ± 0.20\\n-0.28\\n—\\n—\\n—\\n19')]"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chunks = split_documents(pdf_documents)\n",
        "chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3650695d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating embeddings for 79 texts...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Batches: 100%|██████████| 3/3 [00:01<00:00,  2.03it/s]\n"
          ]
        }
      ],
      "source": [
        "texts= [doc.page_content for doc in chunks]\n",
        "\n",
        "\n",
        "#generate enbeddings\n",
        "\n",
        "embeddings = embedding_manager.generate_embeddings(texts)\n",
        "\n",
        "#store in vectordb\n",
        "\n",
        "vector_store.add_doc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a70531d1",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
